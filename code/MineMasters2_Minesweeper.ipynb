{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 최종본에서 cnn pooling layer 삭제 버전"
      ],
      "metadata": {
        "id": "18zqim7sthTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QGe1JOJx8DX"
      },
      "source": [
        "# Minesweeper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaecOabPgaM2"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrX12uf7gZ9A"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import os, sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORyl-kK6h1ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b92e782-3989-4abf-a5a5-14c6de653386"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uezn9MpK7uy4"
      },
      "source": [
        "## Env_new"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.grid_size_X = 9\n",
        "        self.grid_size_Y = 9\n",
        "        self.num_mines = 10\n",
        "\n",
        "        self.minefield = np.zeros((self.grid_size_X, self.grid_size_Y), dtype=int)\n",
        "\n",
        "        self.playerfield = np.full((self.grid_size_X, self.grid_size_Y), 9, dtype=int)\n",
        "\n",
        "        self.state_size = self.minefield.size\n",
        "\n",
        "        self.explode = False\n",
        "        self.done = False\n",
        "        self.first_move = True\n",
        "        self.visited = set()\n",
        "\n",
        "        self.rewards = {'explode' : -1, 'noprogress' : -0.1,'progress' : 0.3, 'guess' : 0.1, 'clear' : 1}\n",
        "\n",
        "    def reset(self):\n",
        "        self.minefield = np.zeros((self.grid_size_X, self.grid_size_Y), dtype=int)\n",
        "        self.playerfield = np.full((self.grid_size_X, self.grid_size_Y), 9, dtype=int)\n",
        "\n",
        "        self.explode = False\n",
        "        self.done = False\n",
        "        self.first_move = True\n",
        "\n",
        "        self.visited = set()\n",
        "\n",
        "        self.place_mines()\n",
        "\n",
        "        return list(self.playerfield)\n",
        "\n",
        "    def place_mines(self):\n",
        "        mines_placed = 0\n",
        "\n",
        "        # num_mines만큼 임의의 좌표에 지뢰 심기\n",
        "        while mines_placed < self.num_mines:\n",
        "            x = random.randint(0, self.grid_size_X - 1)\n",
        "            y = random.randint(0, self.grid_size_Y - 1)\n",
        "\n",
        "            if self.minefield[x, y] == 0:\n",
        "                self.minefield[x, y] = -1\n",
        "                mines_placed += 1\n",
        "\n",
        "        # 지뢰 없는 좌표: 인접 지뢰 개수 세기\n",
        "        for x in range(self.grid_size_X):\n",
        "            for y in range(self.grid_size_Y):\n",
        "                if self.minefield[x, y] == -1:\n",
        "                    continue\n",
        "                self.minefield[x, y] = self.count_adjacent_mines(x, y)\n",
        "\n",
        "    def count_adjacent_mines(self, x, y):\n",
        "        count = 0\n",
        "        # (x,y) 주변 지뢰 개수\n",
        "        for i in range(max(0, x - 1), min(self.grid_size_X, x + 2)):\n",
        "            for j in range(max(0, y - 1), min(self.grid_size_Y, y + 2)):\n",
        "                if (i, j) != (x, y) and self.minefield[i, j] == -1:\n",
        "                    count += 1\n",
        "        return count\n",
        "\n",
        "    def count_adjacent_hidden(self, x, y):\n",
        "        count = 0\n",
        "        # (x,y) 주변 hidden tile 개수\n",
        "        for i in range(max(0, x - 1), min(self.grid_size_X, x + 2)):\n",
        "            for j in range(max(0, y - 1), min(self.grid_size_Y, y + 2)):\n",
        "                if (i, j) != (x, y) and self.playerfield[i, j] == 9:\n",
        "                    count += 1\n",
        "        return count\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = divmod(action, self.grid_size_X)\n",
        "\n",
        "        reward = 0\n",
        "        done = False\n",
        "\n",
        "        # explode: 지뢰 선택 시 done\n",
        "        if self.minefield[x, y] == -1:\n",
        "            self.playerfield[x, y] = self.minefield[x, y]\n",
        "            self.explode = True\n",
        "            done = True\n",
        "            reward = self.rewards['explode']\n",
        "\n",
        "        # 지뢰를 선택하지 않은 경우\n",
        "        else:\n",
        "          # noprogress: 선택한 좌표 (x,y)가 이미 방문된 경우\n",
        "            if (x, y) in self.visited:\n",
        "                reward = self.rewards['noprogress']\n",
        "          # 선택한 좌표 (x, y)가 처음 방문된 경우\n",
        "            else:\n",
        "                self.playerfield[x, y] = self.minefield[x, y]\n",
        "                self.visited.add((x,y))\n",
        "                # 가장자리 타일\n",
        "                if x in [0, 8] or y in [0, 8]:\n",
        "                    # guess\n",
        "                    if self.count_adjacent_hidden(x, y) == 5:\n",
        "                        reward = self.rewards['guess']\n",
        "                    # progress\n",
        "                    else:\n",
        "                        reward = self.rewards['progress']\n",
        "                # 꼭짓점 타일\n",
        "                elif x in [0, 8] and y in [0, 8]:\n",
        "                    # guess\n",
        "                    if self.count_adjacent_hidden(x, y) == 3:\n",
        "                        reward = self.rewards['guess']\n",
        "                    # progress\n",
        "                    else:\n",
        "                        reward = self.rewards['progress']\n",
        "                # 중심부 타일\n",
        "                else:\n",
        "                    if self.count_adjacent_hidden(x, y) == 8:\n",
        "                        reward = self.rewards['guess']\n",
        "                    # progress\n",
        "                    else:\n",
        "                        reward = self.rewards['progress']\n",
        "                # open한 타일이 0이면 주위 타일 open\n",
        "                if self.playerfield[x, y] == 0:\n",
        "                  self.auto_reveal_tiles(x, y)\n",
        "\n",
        "            # clear: 모든 hidden 타일이 지뢰만 남아 있는 경우 승리\n",
        "            if np.count_nonzero(self.playerfield == 9) == self.num_mines:\n",
        "                done = True\n",
        "                reward = self.rewards['clear']\n",
        "\n",
        "        self.done = done\n",
        "        next_state = self.playerfield\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def check_boundary(self, x, y):\n",
        "        return 0 <= x < self.grid_size_X and 0 <= y < self.grid_size_Y\n",
        "\n",
        "    def auto_reveal_tiles(self, x, y):\n",
        "        queue = deque([(x, y)])\n",
        "\n",
        "        while queue:\n",
        "            cx, cy = queue.popleft()\n",
        "            self.visited.add((cx, cy))\n",
        "            self.playerfield[cx, cy] = self.minefield[cx, cy]\n",
        "\n",
        "            # (cx, cy) 주변 8개 타일 확인\n",
        "            if self.minefield[cx, cy] == 0: # 방문하지 않았으면 open\n",
        "                for dx in [-1, 0, 1]:\n",
        "                    for dy in [-1, 0, 1]:\n",
        "                        nx, ny = cx + dx, cy + dy\n",
        "                        # 인덱스가 게임판 범위 내에 있는지 확인\n",
        "                        if self.check_boundary(nx, ny) and (nx, ny) not in self.visited and (nx, ny) not in queue:  # nonvisited 주위 타일 큐에 추가\n",
        "                            queue.append((nx, ny))\n",
        "\n",
        "    def render(self):\n",
        "        for x in range(self.grid_size_X):\n",
        "            for y in range(self.grid_size_Y):\n",
        "                tile = self.playerfield[x, y]\n",
        "                if tile == 9:\n",
        "                    print('.', end=' ')\n",
        "                elif tile == -1:\n",
        "                    print('X', end=' ')\n",
        "                else:\n",
        "                    print(tile, end=' ')\n",
        "                if y == self.grid_size_Y - 1:\n",
        "                    print()\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "TH1y5DcbareE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OyKrxPmfwGB"
      },
      "source": [
        "## Net"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, action_size):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=2, bias=False)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv4 = nn.Conv2d(64, action_size, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.mean(x, dim=[2, 3])  # Global Average Pooling\n",
        "        return x"
      ],
      "metadata": {
        "id": "GoiKUjEk_4bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtH2pv9phoPR"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76EdcMFIVOl6"
      },
      "outputs": [],
      "source": [
        "DISCOUNT_FACTOR = 0.1\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "EPSILON = 0.99\n",
        "EPSILON_DECAY = 0.9999\n",
        "EPSILON_MIN = 0.01\n",
        "\n",
        "TARGET_UPDATE_COUNTER = 0\n",
        "UPDATE_TARGET_EVERY = 5\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "TRAIN_START = 1000\n",
        "MAX_LEN = 50000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxhEFv0wJWeK"
      },
      "outputs": [],
      "source": [
        "class MineSweeper(nn.Module):\n",
        "    def __init__(self, state_size, action_size, grid_size_X, grid_size_Y, environment):\n",
        "        super(MineSweeper, self).__init__()\n",
        "        self.render = False\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.grid_size_X = grid_size_X\n",
        "        self.grid_size_Y = grid_size_Y\n",
        "\n",
        "        self.environment = environment\n",
        "\n",
        "        self.discount_factor = DISCOUNT_FACTOR\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.epsilon = EPSILON\n",
        "        self.epsilon_decay = EPSILON_DECAY\n",
        "        self.epsilon_min = EPSILON_MIN\n",
        "\n",
        "        self.target_update_counter = TARGET_UPDATE_COUNTER\n",
        "        self.update_target_every = UPDATE_TARGET_EVERY\n",
        "\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.train_start = TRAIN_START\n",
        "        self.maxlen = MAX_LEN\n",
        "        self.minlen = MIN_LEN\n",
        "\n",
        "        self.memory = deque(maxlen=self.maxlen)\n",
        "\n",
        "        self.model = Net(self.action_size).to(device)\n",
        "        self.target_model = Net(self.action_size).to(device)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.CyclicLR(optimizer=self.optimizer, base_lr=0.0001, max_lr=0.1, step_size_up=10000, mode='exp_range')\n",
        "\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = np.array(state).reshape(1, 1, self.grid_size_X, self.grid_size_Y)\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            action = random.randrange(self.action_size)\n",
        "        else:\n",
        "            q_value = self.model(state)\n",
        "            self.q_value = q_value.detach().cpu().numpy().flatten()\n",
        "            action = torch.argmax(q_value).item()\n",
        "\n",
        "        return action\n",
        "\n",
        "    def append_sample(self, state, action, reward, next_state, done):\n",
        "        state = state\n",
        "        next_state = next_state\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def train_model(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "        states = np.array(states)\n",
        "        actions = np.array(actions)\n",
        "        rewards = np.array(rewards)\n",
        "        next_states = np.array(next_states)\n",
        "        dones = np.array(dones)\n",
        "\n",
        "        states = states.reshape(self.batch_size, 1, self.grid_size_X, self.grid_size_Y)\n",
        "        next_states = next_states.reshape(self.batch_size, 1, self.grid_size_X, self.grid_size_Y)\n",
        "\n",
        "        states = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "        actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
        "\n",
        "        pred = self.model(states)\n",
        "        target_pred = self.target_model(next_states).max(1)[0].detach()\n",
        "\n",
        "        targets = rewards + (1 - dones) * self.discount_factor * target_pred\n",
        "\n",
        "        pred = pred.gather(1, actions.unsqueeze(1))\n",
        "        trg = targets.unsqueeze(1)\n",
        "\n",
        "        loss = self.loss(pred, trg)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "\n",
        "        self.target_update_counter += 1\n",
        "        if self.target_update_counter >= self.update_target_every:\n",
        "            self.target_update_counter = 0\n",
        "            self.update_target_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIczsrSM7uL-"
      },
      "source": [
        "##모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iub-2cJX7sde"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, epoch, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, path)\n",
        "\n",
        "def load_checkpoint(model, optimizer, path):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    return model, optimizer, epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxEPDAkovjeb"
      },
      "source": [
        "## 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxW0RizSdecq"
      },
      "outputs": [],
      "source": [
        "def plot_training_results(episodes, episodes_N, scores, win_rates, timesteps):\n",
        "    fig, axs = plt.subplots(3, figsize=(12, 18))\n",
        "\n",
        "    # 에피소드 점수\n",
        "    axs[0].plot(episodes, scores, label='Score')\n",
        "    axs[0].set_xlabel('Episode')\n",
        "    axs[0].set_ylabel('Score')\n",
        "    axs[0].set_title('Episode Scores')\n",
        "    axs[0].legend()\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    # 승률\n",
        "    axs[1].plot(episodes_N, win_rates, label='Win Rate', color='orange')\n",
        "    axs[1].set_xlabel('Episode')\n",
        "    axs[1].set_ylabel('Win Rate (%)')\n",
        "    axs[1].set_title('Win Rate over Episodes')\n",
        "    axs[1].legend()\n",
        "    axs[1].grid(True)\n",
        "\n",
        "    # 타임스텝\n",
        "    axs[2].plot(episodes, timesteps, label='Timesteps', color='green')\n",
        "    axs[2].set_xlabel('Episode')\n",
        "    axs[2].set_ylabel('Timesteps')\n",
        "    axs[2].set_title('Timesteps per Episode')\n",
        "    axs[2].legend()\n",
        "    axs[2].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_q_values(q_values, grid_size_X, grid_size_Y):\n",
        "    q_values_grid = q_values.reshape(grid_size_X, grid_size_Y)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(q_values_grid, cmap='viridis', interpolation='none')\n",
        "    plt.colorbar()\n",
        "    plt.title(\"Q-values\")\n",
        "\n",
        "    for i in range(grid_size_X):\n",
        "        for j in range(grid_size_Y):\n",
        "            plt.text(j, i, f'{q_values_grid[i, j]:.2f}', ha='center', va='center', color='white', fontsize=8, fontweight='bold')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzCr_EY0kL5c"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZC3wJnpgBQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8becfde0-7432-40db-c282-145beb61cb91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 499 | time step: 12\n",
            "episode score: 0.10000000000000009 | epsilon: 0.9417\n",
            "\n",
            "<last 500 episode> score: -0.80 | win rate: 0.00%\n",
            "\n",
            "wins: 0.0\n",
            "\n",
            "length of memory: 4895.0\n",
            "\n",
            "0 1 . . . 1 . . . \n",
            "0 2 . . 2 . . . . \n",
            "1 2 . . . . . . . \n",
            ". . . . . . . . 1 \n",
            "1 1 2 . . . . . . \n",
            "0 0 1 X 2 2 2 2 1 \n",
            "0 0 1 1 1 0 0 0 0 \n",
            "1 1 1 0 0 0 0 0 0 \n",
            "1 . 1 0 0 0 0 0 0 \n",
            "\n",
            "\n",
            "chosen_coordinate: [(5, 5), (8, 0), (5, 0), (0, 0), (4, 1), (1, 4), (3, 8), (7, 2), (6, 7), (0, 0), (0, 5), (5, 3)]\n",
            "reward per time step: [0.1, 0.3, 0.1, 0.3, -0.1, 0.1, 0.1, -0.1, 0.1, -0.1, 0.3, -1]\n",
            "--------------------------------------------------\n",
            "episode: 999 | time step: 2\n",
            "episode score: -0.9 | epsilon: 0.8958\n",
            "\n",
            "<last 500 episode> score: -0.80 | win rate: 0.00%\n",
            "\n",
            "wins: 0.0\n",
            "\n",
            "length of memory: 9702.0\n",
            "\n",
            ". . . X . . . . . \n",
            ". . . . . . . . . \n",
            ". . . . . . . . . \n",
            ". . . . . . . . . \n",
            ". . . . . . . . . \n",
            ". . . . . . . . . \n",
            ". . . . . . . . . \n",
            ". . . . . . . . . \n",
            ". . . . . . 1 . . \n",
            "\n",
            "\n",
            "chosen_coordinate: [(8, 6), (0, 3)]\n",
            "reward per time step: [0.1, -1]\n",
            "--------------------------------------------------\n",
            "episode: 1499 | time step: 17\n",
            "episode score: -0.6 | epsilon: 0.8521\n",
            "\n",
            "<last 500 episode> score: -0.80 | win rate: 0.00%\n",
            "\n",
            "wins: 0.0\n",
            "\n",
            "length of memory: 14361.0\n",
            "\n",
            "1 . 2 . . . . 1 0 \n",
            ". . . . . 2 1 1 0 \n",
            ". . 2 1 1 1 0 0 0 \n",
            ". . 1 0 0 0 0 0 0 \n",
            "1 1 1 0 0 0 0 0 0 \n",
            "0 0 0 0 1 1 1 0 0 \n",
            "0 0 0 0 1 X 2 1 1 \n",
            "1 1 0 1 2 . . . 1 \n",
            ". 1 0 1 . . 1 1 . \n",
            "\n",
            "\n",
            "chosen_coordinate: [(0, 2), (7, 2), (6, 8), (7, 8), (6, 8), (4, 6), (1, 7), (0, 0), (8, 7), (6, 3), (8, 6), (7, 1), (0, 0), (2, 4), (6, 3), (3, 3), (6, 5)]\n",
            "reward per time step: [0.1, 0.1, -0.1, 0.3, -0.1, -0.1, -0.1, 0.3, 0.3, -0.1, 0.3, -0.1, -0.1, -0.1, -0.1, -0.1, -1]\n",
            "--------------------------------------------------\n",
            "episode: 1999 | time step: 10\n",
            "episode score: -0.8999999999999999 | epsilon: 0.8105\n",
            "\n",
            "<last 500 episode> score: -0.80 | win rate: 0.00%\n",
            "\n",
            "wins: 0.0\n",
            "\n",
            "length of memory: 19470.0\n",
            "\n",
            "0 0 0 0 0 1 . . . \n",
            "1 2 2 2 1 2 . . . \n",
            ". . . . . . . . . \n",
            ". . . . . . . . . \n",
            ". . . . . . . . 1 \n",
            ". . . X 2 1 1 1 . \n",
            ". . 3 1 1 0 0 1 1 \n",
            ". . 2 0 0 0 0 0 0 \n",
            ". . 1 0 0 0 0 0 0 \n",
            "\n",
            "\n",
            "chosen_coordinate: [(6, 7), (7, 2), (4, 8), (8, 7), (0, 4), (0, 0), (7, 3), (0, 4), (0, 0), (5, 3)]\n",
            "reward per time step: [0.1, 0.1, 0.1, 0.1, 0.1, -0.1, -0.1, -0.1, -0.1, -1]\n",
            "--------------------------------------------------\n",
            "episode: 2499 | time step: 25\n",
            "episode score: -2.0 | epsilon: 0.7710\n",
            "\n",
            "<last 500 episode> score: -0.80 | win rate: 0.00%\n",
            "\n",
            "wins: 0.0\n",
            "\n",
            "length of memory: 24742.0\n",
            "\n",
            "0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 0 0 0 0 \n",
            "1 1 1 1 1 1 1 1 0 \n",
            ". 1 1 X . . . 1 0 \n",
            ". . . . . . . 1 0 \n",
            ". . . . . . . 1 0 \n",
            ". . . . 2 . . 1 0 \n",
            "1 2 1 2 . . . 2 1 \n",
            "0 0 0 1 . . . . . \n",
            "\n",
            "\n",
            "chosen_coordinate: [(0, 4), (0, 0), (0, 1), (3, 1), (0, 0), (3, 1), (0, 0), (3, 2), (0, 0), (0, 3), (2, 1), (0, 0), (0, 0), (6, 4), (0, 0), (0, 0), (8, 2), (0, 0), (0, 0), (1, 4), (7, 8), (0, 0), (0, 0), (0, 0), (3, 3)]\n",
            "reward per time step: [0.1, -0.1, -0.1, 0.3, -0.1, -0.1, -0.1, 0.3, -0.1, -0.1, -0.1, -0.1, -0.1, 0.1, -0.1, -0.1, 0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -1]\n",
            "--------------------------------------------------\n",
            "episode: 2999 | time step: 19\n",
            "episode score: -1.0 | epsilon: 0.7334\n",
            "\n",
            "<last 500 episode> score: -0.80 | win rate: 0.00%\n",
            "\n",
            "wins: 0.0\n",
            "\n",
            "length of memory: 30233.0\n",
            "\n",
            "1 . . 1 0 0 0 0 0 \n",
            ". . . 2 1 0 0 0 0 \n",
            ". . . . 1 0 0 0 0 \n",
            ". . . 2 1 0 0 0 0 \n",
            ". . 2 1 0 0 0 0 0 \n",
            ". . . 1 1 1 1 1 1 \n",
            ". 1 . . X . . . . \n",
            ". 1 . . . . . . . \n",
            ". . . 1 . . . . . \n",
            "\n",
            "\n",
            "chosen_coordinate: [(3, 6), (2, 6), (4, 6), (8, 3), (3, 6), (6, 1), (0, 0), (0, 0), (4, 2), (5, 5), (6, 1), (0, 0), (0, 0), (2, 7), (0, 0), (2, 8), (0, 0), (7, 1), (6, 4)]\n",
            "reward per time step: [0.1, -0.1, -0.1, 0.1, -0.1, 0.1, 0.3, -0.1, 0.3, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, 0.3, -1]\n",
            "--------------------------------------------------\n",
            "episode: 3499 | time step: 30\n",
            "episode score: -2.900000000000001 | epsilon: 0.6976\n",
            "\n",
            "<last 500 episode> score: -0.80 | win rate: 0.00%\n",
            "\n",
            "wins: 0.0\n",
            "\n",
            "length of memory: 35585.0\n",
            "\n",
            "0 0 0 0 0 0 0 0 0 \n",
            "0 0 0 0 0 1 2 2 1 \n",
            "0 0 0 0 1 2 . . . \n",
            "1 1 1 0 1 . 3 2 1 \n",
            ". X 1 0 1 1 1 0 0 \n",
            ". . 1 0 0 0 0 0 0 \n",
            ". 1 1 1 1 0 0 1 1 \n",
            ". . 1 . 2 2 2 2 . \n",
            ". . . . . . . . . \n",
            "\n",
            "\n",
            "chosen_coordinate: [(1, 0), (1, 2), (7, 2), (0, 0), (0, 0), (6, 1), (1, 0), (3, 7), (1, 4), (0, 0), (2, 5), (5, 6), (4, 6), (0, 3), (7, 5), (2, 5), (3, 3), (3, 6), (0, 0), (1, 5), (5, 4), (2, 3), (5, 2), (0, 0), (7, 6), (2, 5), (4, 7), (5, 3), (2, 4), (4, 1)]\n",
            "reward per time step: [0.1, -0.1, 0.3, -0.1, -0.1, 0.3, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -1]\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "env = Environment()\n",
        "\n",
        "state_size = env.state_size\n",
        "action_size = env.state_size\n",
        "grid_size_X = env.grid_size_X\n",
        "grid_size_Y = env.grid_size_Y\n",
        "\n",
        "agent = MineSweeper(state_size, action_size, grid_size_X, grid_size_Y, env)\n",
        "\n",
        "EPISODES = 100000\n",
        "RENDER_PROCESS = False\n",
        "RENDER_END = False\n",
        "\n",
        "total_moves = []\n",
        "scores = np.zeros(EPISODES)\n",
        "length_memory = np.zeros(EPISODES)\n",
        "wins = np.zeros(EPISODES)\n",
        "episodes = np.zeros(EPISODES)\n",
        "timesteps = np.zeros(EPISODES)\n",
        "win_rates = {}\n",
        "\n",
        "N = 500\n",
        "CHECKPOINT_INTERVAL = 10000\n",
        "\n",
        "for epi in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "    time_step = 0\n",
        "    actions = []\n",
        "    rewards = []\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    last_loss = None\n",
        "\n",
        "    while not done and time_step <= 71:\n",
        "        time_step += 1\n",
        "        if env.first_move:\n",
        "            mine_state = env.minefield.flatten()\n",
        "            first_action = random.randint(0, len(mine_state)-1)\n",
        "            first_state = mine_state[first_action]\n",
        "            while first_state == -1:\n",
        "                first_action = random.randint(0, len(mine_state)-1)\n",
        "                first_state = mine_state[first_action]\n",
        "            action = first_action\n",
        "            env.first_move = False\n",
        "        else:\n",
        "            action = agent.get_action(state)\n",
        "\n",
        "        next_state, reward, done = env.step(action)\n",
        "        score += reward\n",
        "\n",
        "        (action_x, action_y) = divmod(action, env.grid_size_X)\n",
        "        actions.append((action_x, action_y))\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # state (신경망의 input) 정규화\n",
        "        scaled_state = (next_state - (-1)) / (8 - (-1))\n",
        "\n",
        "        agent.append_sample(state, action, reward, scaled_state, done)\n",
        "\n",
        "        if len(agent.memory) >= agent.train_start:\n",
        "            agent.train_model()\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    scores[epi] = score\n",
        "    timesteps[epi] = time_step\n",
        "\n",
        "    # 에피소드가 끝날 때 승리 여부를 기록\n",
        "    if env.explode or time_step > 71:\n",
        "        wins[epi] = 0\n",
        "    elif not env.explode:\n",
        "        wins[epi] = 1\n",
        "        print(f\"episode: {epi}\")\n",
        "        print(f\"episode score: {score}\")\n",
        "        print(f\"time step: {time_step}\")\n",
        "        print(f\"epsilon: {agent.epsilon:.4f}\")\n",
        "        env.render()\n",
        "\n",
        "    if agent.epsilon > agent.epsilon_min:\n",
        "        agent.epsilon *= agent.epsilon_decay\n",
        "\n",
        "    if (epi+1) % N == 0:\n",
        "        scores_N = np.median(scores[max(0, epi-N+1):epi+1])  # 마지막 N개의 요소에 대한 중간값 계산\n",
        "        win_rate = np.mean(wins[max(0, epi-N+1):epi+1]) * 100  # 마지막 N개의 에피소드에 대한 승률 계산\n",
        "        win_rates[epi] = win_rate\n",
        "        length_memory[epi] = len(agent.memory)\n",
        "        print(f\"episode: {epi:3d} | time step: {time_step}\")\n",
        "        print(f\"episode score: {score} | epsilon: {agent.epsilon:.4f}\\n\")\n",
        "        print(f\"<last {N} episode> score: {scores_N:.2f} | win rate: {win_rate:.2f}%\\n\")\n",
        "        print(f\"wins: {np.sum(wins[max(0, epi-N+1):epi+1])}\\n\")\n",
        "        print(f\"length of memory: {length_memory[epi]}\\n\")\n",
        "        env.render()\n",
        "        print(f\"chosen_coordinate: {actions}\")\n",
        "        print(f\"reward per time step: {rewards}\")\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "    # 매 CHECKPOINT_INTERVAL마다 모델 저장\n",
        "    if (epi+1) % CHECKPOINT_INTERVAL == 0:\n",
        "        checkpoint_path = f\"checkpoint_{epi}.tar\"\n",
        "        save_checkpoint(agent, agent.optimizer, epi, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at episode {epi} to {checkpoint_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 완료 후 Q-value 시각화\n",
        "q_values = agent.q_value\n",
        "visualize_q_values(q_values, grid_size_X, grid_size_Y)"
      ],
      "metadata": {
        "id": "Z8DCvGQc1EXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = list(range(0, EPISODES))"
      ],
      "metadata": {
        "id": "H_HP2yz51G6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_results(episodes, [np.median(scores[i-N:i]) for i in episodes],\n",
        "                      [np.mean(wins[i-N:i]) * 100 for i in episodes],\n",
        "                      [np.median(timesteps[i-N:i]) for i in episodes])"
      ],
      "metadata": {
        "id": "BbCg9lMK0RuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrP5vFxL7eAw"
      },
      "source": [
        "##저장한 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht_AdgFn7YGf"
      },
      "outputs": [],
      "source": [
        "# # 체크포인트 불러오기\n",
        "# checkpoint_path = 'checkpoint_5000.tar'  # 예를 들어 5000번째 에피소드 체크포인트\n",
        "# agent, optimizer, start_epoch, last_loss = load_checkpoint(agent, optimizer, checkpoint_path)\n",
        "\n",
        "# print(f\"Checkpoint loaded from {checkpoint_path}. Starting from epoch {start_epoch}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgQWds-fVPHD"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "g-f9PdrwvUFC"
      },
      "outputs": [],
      "source": [
        "# TEST_EPI = 1000\n",
        "\n",
        "# test_scores = np.zeros(TEST_EPI)\n",
        "# test_wins = np.zeros(TEST_EPI)\n",
        "# test_timesteps = np.zeros(TEST_EPI)\n",
        "\n",
        "# for i in range(TEST_EPI):\n",
        "#     state = env.reset()\n",
        "#     done = False\n",
        "#     score = 0\n",
        "#     steps = 0\n",
        "#     agent.epsilon = 0\n",
        "\n",
        "#     while not done:\n",
        "#         action = agent.get_action(state)\n",
        "#         next_state, reward, done = env.step(action)\n",
        "#         score += reward\n",
        "#         steps += 1\n",
        "#         state = next_state\n",
        "\n",
        "#     if done and not env.explode:\n",
        "#         test_wins[i] = 1\n",
        "#     else:\n",
        "#         test_wins[i] = 0\n",
        "\n",
        "#     test_scores[i] = score\n",
        "#     test_timesteps[i] = steps\n",
        "\n",
        "#     if (i+1) % 100 == 0:\n",
        "#         print(f\"Episode {i+1} | Score: {score}, Steps: {steps}\")\n",
        "#         env.render()\n",
        "#         visualize_q_values(agent.q_value, agent.grid_size_X, agent.grid_size_Y) # Q-values 시각화\n",
        "#         print(\"\\n\")\n",
        "#         print(\"--------------------------------------------------------------------------------------\")\n",
        "#         print(\"\\n\")\n",
        "\n",
        "# # 테스트 승률 출력\n",
        "# test_win_rate = np.mean(test_wins) * 100\n",
        "# print(f\"Test Win Rate: {test_win_rate:.2f}%\")\n",
        "\n",
        "# test_epi = list(range(TEST_EPI))\n",
        "# fig, axs = plt.subplots(2, figsize=(12, 18))\n",
        "# # 에피소드 점수\n",
        "# axs[0].plot(test_epi, test_scores, label='Score')\n",
        "# axs[0].set_xlabel('Episode')\n",
        "# axs[0].set_ylabel('Score')\n",
        "# axs[0].set_title('Episode Scores')\n",
        "# axs[0].legend()\n",
        "# axs[0].grid(True)\n",
        "\n",
        "# # 타임스텝\n",
        "# axs[1].plot(test_epi, test_timesteps, label='Timesteps', color='green')\n",
        "# axs[1].set_xlabel('Episode')\n",
        "# axs[1].set_ylabel('Timesteps')\n",
        "# axs[1].set_title('Timesteps per Episode')\n",
        "# axs[1].legend()\n",
        "# axs[1].grid(True)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "FIczsrSM7uL-",
        "WxEPDAkovjeb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}