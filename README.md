# 24-1-MineMasters-02
[ 24-1 /  MineMasters / Team 02 ]  
ğŸ‘©â€ğŸ’» ì´ì •ì—°, ì†ì£¼í˜„
***
# ëª©ì°¨
[Environment](#Environment)

[Net](#Net)

[Agent](#Agent)

[Train](#Train)

[ì‹œë„í•œ ë°©ë²•ë¡  ë¶„ì„ ì•„ì´ë””ì–´ ë° ê²°ê³¼](#-ì‹œë„í•œ-ë°©ë²•ë¡ -ë¶„ì„-ì•„ì´ë””ì–´-ë°-ê²°ê³¼)

[ì½”ë“œ ì†ë„ ê°œì„ ](#ì½”ë“œ-ì†ë„-ê°œì„ )

[ìµœê³  ì„±ëŠ¥ì´ ë‚˜ì˜¨ ë°©ë²•ë¡  (ëª¨ë¸)](#-ìµœê³ -ì„±ëŠ¥ì´-ë‚˜ì˜¨-ë°©ë²•ë¡ -(ëª¨ë¸))

[ë¬¸ì œ í•´ê²° ë° ê°œì„ í•œ ì ](#-ë¬¸ì œ-í•´ê²°-ë°-ê°œì„ í•œ-ì )

***

 # Environment

## Attributes

- **`grid_size_X`** , **`grid_size_Y`** : ê²Œì„íŒì˜ ê°€ë¡œ, ì„¸ë¡œ í¬ê¸°
- **`num_mines`** : ì§€ë¢° ê°œìˆ˜
- **state**
    - **`state_size`**: ì§€ë¢°íŒ í¬ê¸°
    - **`minefield`**  : ì§€ë¢° ì°¾ê¸° ê²Œì„ ìƒí™©ì´ ì €ì¥ëœ ì• íŠ¸ë¦¬ë·°íŠ¸(ì •ë‹µì§€)
        
        - **type:** np.array
        - **êµ¬ì¡°:** `grid_size_X` x `grid_size_Y` í¬ê¸°ì˜ 2D NumPy ë°°ì—´
        - ì´ˆê¸°ê°’: ëª¨ë“  ì…€ì´ 0 -> ì´í›„ reset() ë‚´ place_mines()ì—ì„œ ì§€ë¢°ë¥¼ ì‹¬ëŠ”ë‹¤. (-1ë¡œ update)
    - **`playerfield`** : í”Œë ˆì´ì–´ê°€ ë³´ëŠ” ì§€ë¢°ë°­ ìƒíƒœ
        
        *-1(mine), 0, 1, 2, 3, 4, 5, 6, 7, 8, 9(hidden)* 
        
        0~8ì€ ì¸ì ‘í•œ ì§€ë¢° ê°œìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
        
        - **type:** np.array
        - **êµ¬ì¡°:** `grid_size_X` x `grid_size_Y` í¬ê¸°ì˜ 2D NumPy ë°°ì—´
        - **ì´ˆê¸°ê°’:** ëª¨ë“  ì…€ì´ 9
- **ê²Œì„ ì§„í–‰ ê´€ë ¨ ë³€ìˆ˜ë“¤**
    - **`explode`**: í”Œë ˆì´ì–´ê°€ ì§€ë¢°ë¥¼ ë°Ÿì•˜ëŠ”ì§€ ì—¬ë¶€
    - **`done`**  : ê²Œì„ì´ ëë‚¬ëŠ”ì§€ ì—¬ë¶€
    - **`first_move`**: í˜„ì¬ ì›€ì§ì„ì´ ì²« ë²ˆì§¸ ì›€ì§ì„ì¸ì§€ ì—¬ë¶€
- ì´ë•Œ **`action`** ì€ ì—ì´ì „íŠ¸ê°€ playerfieldì—ì„œ ì„ íƒí•œ ì¢Œí‘œë¡œ ì •ì˜í•œë‹¤.
- ë³´ìƒ **`rewards`**
    | explode | noprogress | guess | progress | clear |
    | --- | --- | --- | --- | --- |
    | -1 | -1 | 0.1 | 0.3 | 1 |
    - 'guess' : 0.1
        - ì£¼ë³€ì„ ë‘˜ëŸ¬ì‹¼ íƒ€ì¼ë“¤ì´ ì „ë¶€ hidden tileì¸ ê²½ìš° (ì•„ë¬´ëŸ° ì •ë³´ ì—†ì´ ì°ì—ˆë‹¤ê³  ê°„ì£¼í•¨)
    - 'progress' : 0.3
        - ì£¼ë³€ì„ ë‘˜ëŸ¬ì‹¼ íƒ€ì¼ ì¤‘ì— í•˜ë‚˜ë¼ë„ openëœ íƒ€ì¼ì´ ìˆëŠ” ê²½ìš°
        - ê°€ì¥ìë¦¬ íƒ€ì¼ì€ ì£¼ë³€ì„ ë‘˜ëŸ¬ì‹¼ íƒ€ì¼ì´ 5ê°œ / ê¼­ì§“ì  íƒ€ì¼ì€ 3ê°œ

## Methods

### 1. **`__init__`**: 
ê²Œì„íŒ í¬ê¸°, minefield í¬ê¸°, playerfield í¬ê¸°, state í¬ê¸°, í­ë°œ ì—¬ë¶€, done ì—¬ë¶€, ì²« ìŠ¤í… ì—¬ë¶€, ë°©ë¬¸ ì¢Œí‘œ set, reward ë°°ì—´ ë“±ì˜ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•œë‹¤.

```python
class Environment:
    def __init__(self):
        self.grid_size_X = 9
        self.grid_size_Y = 9
        self.num_mines = 10

        self.minefield = np.zeros((self.grid_size_X, self.grid_size_Y), dtype=int)

        self.playerfield = np.full((self.grid_size_X, self.grid_size_Y), 9, dtype=int)

        self.state_size = self.minefield.size

        self.explode = False
        self.done = False
        self.first_move = True
        self.visited = set()

        self.rewards = {'explode' : -1, 'noprogress' : -0.1,'progress' : 0.3, 'guess' : 0.1, 'clear' : 1}
```

### 2. **`reset`**: 
ì—í”¼ì†Œë“œë¥¼ ì´ˆê¸° ìƒíƒœë¡œ ë˜ëŒë¦¬ëŠ” ì—­í• 
    
    ìƒˆë¡œìš´ ê²Œì„ì— í•„ìš”í•œ ì§€ë¢°ë¥¼ ë°°ì¹˜í•˜ê³ , ìƒˆë¡œìš´ playerfield ë¥¼ ì œê³µí•œë‹¤.
    
    - `place_mines`  : ì§€ë¢° ê°œìˆ˜ë§Œí¼ ì„ì˜ì˜ ì¢Œí‘œì— ì§€ë¢°ë¥¼ ì‹¬ì€ í›„, ì§€ë¢°ê°€ ì—†ëŠ” ì¢Œí‘œì— ëŒ€í•´ì„œëŠ” ì¸ì ‘í•œ ì§€ë¢°ê°œìˆ˜ë¥¼ playerfieldì— updateí•œë‹¤.
    - `count_adjacent_mines` : ì¸ì ‘í•œ ì§€ë¢° ê°œìˆ˜ë¥¼ ì„¸ëŠ” ë©”ì†Œë“œ

```python
    def reset(self):
        self.minefield = np.zeros((self.grid_size_X, self.grid_size_Y), dtype=int)
        self.playerfield = np.full((self.grid_size_X, self.grid_size_Y), 9, dtype=int)

        self.explode = False
        self.done = False
        self.first_move = True

        self.visited = set()

        self.place_mines()

        return list(self.playerfield)
```
```python
    def place_mines(self):
        mines_placed = 0

        # num_minesë§Œí¼ ì„ì˜ì˜ ì¢Œí‘œì— ì§€ë¢° ì‹¬ê¸°
        while mines_placed < self.num_mines:
            x = random.randint(0, self.grid_size_X - 1)
            y = random.randint(0, self.grid_size_Y - 1)

            if self.minefield[x, y] == 0:
                self.minefield[x, y] = -1
                mines_placed += 1

        # ì§€ë¢° ì—†ëŠ” ì¢Œí‘œ: ì¸ì ‘ ì§€ë¢° ê°œìˆ˜ ì„¸ê¸°
        for x in range(self.grid_size_X):
            for y in range(self.grid_size_Y):
                if self.minefield[x, y] == -1:
                    continue
                self.minefield[x, y] = self.count_adjacent_mines(x, y)

    def count_adjacent_mines(self, x, y):
        count = 0
        # (x,y) ì£¼ë³€ ì§€ë¢° ê°œìˆ˜
        for i in range(max(0, x - 1), min(self.grid_size_X, x + 2)):
            for j in range(max(0, y - 1), min(self.grid_size_Y, y + 2)):
                if (i, j) != (x, y) and self.minefield[i, j] == -1:
                    count += 1
        return count
```

### 3. **`step`**: 
ì—ì´ì „íŠ¸ê°€ í™˜ê²½ì—ì„œ actionì„ í•œ ë‹¨ê³„ ìˆ˜í–‰í•  ë•Œë§ˆë‹¤ í˜¸ì¶œ
    - next_state, reward, done ë°˜í™˜
    - ë™ì‘ ê³¼ì •
        1. 1ì°¨ì› ì¸ë±ìŠ¤ `action` ë¥¼ 2ì°¨ì› ì¢Œí‘œë¡œ ë³€í™˜í•œë‹¤. 
            
            (minefield, playerfield ëª¨ë‘ 2ì°¨ì›ì´ê¸° ë•Œë¬¸ì—)
            
        2. ì„ íƒí•œ ì¢Œí‘œ (x,y)ê°€ ì§€ë¢°ì¸ì§€ ì—¬ë¶€ë¥¼ minefieldì—ì„œ í™•ì¸
            1. ì§€ë¢°(-1)ì¸ ê²½ìš° 
                1. done, explode, reward ë¡œ ê°ê° True, True, rewards[â€™explodeâ€™] ë°˜í™˜
                2. ê²Œì„ íŒ¨ë°°
            2. ì§€ë¢°(-1)ê°€ ì•„ë‹Œ ê²½ìš°
                1. ì„ íƒí•œ ì¢Œí‘œê°€ ì´ë¯¸ openëœ ì¢Œí‘œì¸ ê²½ìš°
                    1. done, explode, reward ë¡œ ê°ê° False, False, rewards[â€™nonprogressâ€™] ë°˜í™˜
                    2. next_state ë¡œ player_field ë°˜í™˜
                2. ì„ íƒí•œ ì¢Œí‘œê°€ ì²˜ìŒ openëœ ì¢Œí‘œì¸ ê²½ìš°
                    1. visited ë°°ì—´ì— íƒ€ì¼ ì¶”ê°€
                    2. íƒ€ì¼ open (playerfieldì˜ ì¢Œí‘œì— minefieldì˜ í•´ë‹¹ì¢Œí‘œ ê°’ì„ ë³µì‚¬)
                        1-1. open í•œ íƒ€ì¼ì´ ì£¼ë³€ì´ ì „ë¶€ hiddenì¸ ê²½ìš°
                            - rewardë¡œ rewards[â€™guessâ€™] ë¶€ì—¬
                            
                        1-2. open í•œ íƒ€ì¼ ì£¼ë³€ì— ì´ë¯¸ openëœ íƒ€ì¼ì´ ìˆëŠ” ê²½ìš°
                            - rewardë¡œ rewards[â€™progressâ€™]  ë¶€ì—¬
              
                        2. open í•œ íƒ€ì¼ì´ 0ì´ë©´ ì£¼ìœ„ íƒ€ì¼ open
                        3. hidden tile(9)ì´ ë‚¨ì•„ìˆëŠ” ê²½ìš° done=False, ë‚¨ì•„ìˆì§€ ì•Šì€ ê²½ìš° done=True ë°˜í™˜
                    	4. next_state ë¡œ playerfield ë°˜í™˜

```python
def step(self, action):
        x, y = divmod(action, self.grid_size_X)

        reward = 0
        done = False

        # explode: ì§€ë¢° ì„ íƒ ì‹œ done
        if self.minefield[x, y] == -1:
            self.playerfield[x, y] = self.minefield[x, y]  # íƒ€ì¼ ì—´ê¸°
            self.explode = True
            done = True
            reward = self.rewards['explode']

        # ì§€ë¢°ë¥¼ ì„ íƒí•˜ì§€ ì•Šì€ ê²½ìš°
        else:
          # noprogress: ì„ íƒí•œ ì¢Œí‘œ (x,y)ê°€ ì´ë¯¸ ë°©ë¬¸ëœ ê²½ìš°
            if (x, y) in self.visited:
                reward = self.rewards['noprogress']
          # ì„ íƒí•œ ì¢Œí‘œ (x, y)ê°€ ì²˜ìŒ ë°©ë¬¸ëœ ê²½ìš°
            else:
                self.playerfield[x, y] = self.minefield[x, y]  # íƒ€ì¼ ì—´ê¸°
                self.visited.add((x,y))
                # ê°€ì¥ìë¦¬ íƒ€ì¼
                if x in [0, 8] or y in [0, 8]:
                    # guess
                    if self.count_adjacent_hidden(x, y) == 5:
                        reward = self.rewards['guess']
                    # progress
                    else:
                        reward = self.rewards['progress']
                # ê¼­ì§“ì  íƒ€ì¼
                elif x in [0, 8] and y in [0, 8]:
                    # guess
                    if self.count_adjacent_hidden(x, y) == 3:
                        reward = self.rewards['guess']
                    # progress
                    else:
                        reward = self.rewards['progress']
                else:
                    if self.count_adjacent_hidden(x, y) == 8:
                        reward = self.rewards['guess']
                    # progress
                    else:
                        reward = self.rewards['progress']
                # opení•œ íƒ€ì¼ì´ 0ì´ë©´ ì£¼ìœ„ íƒ€ì¼ open
                if self.playerfield[x, y] == 0:
                  self.auto_reveal_tiles(x, y)  # (x, y) ì£¼ë³€ íƒ€ì¼ ì—´ê¸°

            # clear: ëª¨ë“  hidden íƒ€ì¼ì´ ì§€ë¢°ë§Œ ë‚¨ì•„ ìˆëŠ” ê²½ìš° ìŠ¹ë¦¬
            if np.count_nonzero(self.playerfield == 9) == self.num_mines:
                done = True
                reward = self.rewards['clear']

        self.done = done
        next_state = self.playerfield
        return next_state, reward, done
```

- íƒ€ì¼ì„ opení•  ë•Œ í•„ìš”í•œ ë©”ì„œë“œ
    - `check_boundary` : opení•  íƒ€ì¼ì´ ê²Œì„íŒì„ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡
    - `auto_reveal_tiles`  : 0ì„ ì„ íƒí•œ ê²½ìš° ì—°ì‡„ì ìœ¼ë¡œ ì£¼ìœ„ íƒ€ì¼ì„ ì „ë¶€ open
        - BFS êµ¬ì¡°
        - ì£¼ë³€ 8ê°œ íƒ€ì¼ì˜ ìˆ«ì í™•ì¸, ê²Œì„íŒì„ ë²—ì–´ë‚˜ì§€ ì•Šê³  ë°©ë¬¸í•˜ì§€ ì•Šì€ ê²½ìš° íì— ì¶”ê°€
     
- ì„ íƒí•œ íƒ€ì¼ ì£¼ë³€ hidden tile ê°œìˆ˜ë¥¼ ì„¸ëŠ” ë°ì— í•„ìš”í•œ ë©”ì„œë“œ
    - `count_adjacent_hidden`

```python
    def check_boundary(self, x, y):
        return 0 <= x < self.grid_size_X and 0 <= y < self.grid_size_Y

    def auto_reveal_tiles(self, x, y):  # BFS
        queue = deque([(x, y)])
        self.visited = set()

        while queue:
            cx, cy = queue.popleft()
            self.visited.add((cx, cy))  # (cx, cy) ë°©ë¬¸ í‘œì‹œ
            self.playerfield[cx, cy] = self.minefield[cx, cy]  # (cx, cy) íƒ€ì¼ ì—´ê¸°
            self.visit_count[(cx, cy)] = self.visit_count.get((cx, cy), 0) + 1  # ë°©ë¬¸ íšŸìˆ˜ ê¸°ë¡

            # (cx, cy) ì£¼ë³€ 8ê°œ íƒ€ì¼ í™•ì¸, ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´ íì— insert
            if self.minefield[cx, cy] == 0: 
                for dx in [-1, 0, 1]:
                    for dy in [-1, 0, 1]:
                        nx, ny = cx + dx, cy + dy
                        # ì¸ë±ìŠ¤ê°€ ê²Œì„íŒ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
                        if self.check_boundary(nx, ny) and (nx, ny) not in self.visited and (nx, ny) not in queue:  # nonvisited ì£¼ìœ„ íƒ€ì¼ íì— ì¶”ê°€
                            queue.append((nx, ny))
```

### 4. **`render`** : 
íŠ¹ì • ì‹œì ì— `playerfield` ê²Œì„íŒì˜ ìƒíƒœë¥¼ render
    - hidden tile: **.**
    - mine: X
    - ë‚˜ë¨¸ì§€: 0~8 (ì¸ì ‘í•œ ì§€ë¢°ìˆ˜)

```python
def render(self):  # ì¸ìˆ˜ ì„¤ì •
        for x in range(self.grid_size_X):
            for y in range(self.grid_size_Y):
                tile = self.playerfield[x, y]
                if tile == 9:
                    print('.', end=' ')
                elif tile == -1:
                    print('X', end=' ')
                else:
                    print(tile, end=' ')
                if y == self.grid_size_Y - 1:
                    print()
        print('\n')
```
***
# Net

***
# Agent

### Hyperparameters

```python
DISCOUNT_FACTOR = 0.9
LEARNING_RATE = 0.001
EPSILON = 1.0
EPSILON_DECAY = 0.999
EPSILON_MIN = 0.01

BATCH_SIZE = 64
TRAIN_START = 1000
MAX_LEN = 5000

TARGET_UPDATE_COUNTER = 0
UPDATE_TARGET_EVERY = 5
```

- **DISCOUNT_FACTOR**: ë¯¸ë˜ ë³´ìƒì˜ í˜„ì¬ ê°€ì¹˜ì— ëŒ€í•œ í• ì¸ ê³„ìˆ˜
- **LEARNING_RATE**: optimizerì— ì ìš©í•  í•™ìŠµë¥ 
- **EPSILON**: íƒí—˜ì„ í•  í™•ë¥ ì„ ì¡°ì ˆí•˜ëŠ” ê°’ (ì´ˆê¸° 1.0)
- **EPSILON_MIN**: (ìµœì†Œ 0.01)
- **EPSILON_DECAY** ë§¤ ì—í”¼ì†Œë“œê°€ ëë‚  ë•Œë§ˆë‹¤ ì…ì‹¤ë¡ ì— ê³±í•˜ì—¬ ì…ì‹¤ë¡  ê°’ì„ ì‘ì•„ì§€ê²Œ í•˜ëŠ” ê°’
- **BATCH_SIZE**: ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì—ì„œ ìƒ˜í”Œë§í•  ë°°ì¹˜ í¬ê¸°
- **TRAIN_START**: í•™ìŠµì„ ì‹œì‘í•  ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì˜ ìµœì†Œ í¬ê¸°
- **MAX_LEN**: ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì˜ ìµœëŒ€ ê¸¸ì´
- **TARGET_UPDATE_EVERY**: íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•  ì£¼ê¸°

### Methods

```python
class MineSweeper(nn.Module):
    def __init__(self, state_size, action_size, grid_size_X, grid_size_Y, environment):
        super(MineSweeper, self).__init__()
        self.render = False

        self.state_size = state_size
        self.action_size = action_size
        self.grid_size_X = grid_size_X
        self.grid_size_Y = grid_size_Y

        self.environment = environment

        self.discount_factor = DISCOUNT_FACTOR
        self.learning_rate = LEARNING_RATE
        self.epsilon = EPSILON
        self.epsilon_decay = EPSILON_DECAY
        self.epsilon_min = EPSILON_MIN

        self.target_update_counter = TARGET_UPDATE_COUNTER
        self.update_target_every = UPDATE_TARGET_EVERY

        self.batch_size = BATCH_SIZE
        self.train_start = TRAIN_START
        self.maxlen = MAX_LEN

        self.memory = deque(maxlen=self.maxlen)

        self.model = Net(self.grid_size_X, self.grid_size_Y, self.action_size).to(device)
        self.target_model = Net(self.grid_size_X, self.grid_size_Y, self.action_size).to(device)
        self.loss = nn.MSELoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        # self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.01, alpha=0.99)
        # self.scheduler = optim.lr_scheduler.CyclicLR(optimizer=self.optimizer, base_lr=1e-5, max_lr=1e-1, step_size_up=10, mode='triangular')
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.99)

        self.update_target_model()
```

- `__init__`: ìƒíƒœ í¬ê¸°, í–‰ë™ í¬ê¸°, í•˜ì´í¼íŒŒë¼ë¯¸í„°, ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬, ëª¨ë¸, íƒ€ê¹ƒ ëª¨ë¸ ë“±ì„ ì´ˆê¸°í™”í•œë‹¤.

```python
    def update_target_model(self):
        self.target_model.load_state_dict(self.model.state_dict())
```

- `update_target_model`: íƒ€ê¹ƒ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•œë‹¤.

```python
    def get_action(self, state):
        state = np.array(state).reshape(1, 1, self.grid_size_X, self.grid_size_Y)
        state = torch.FloatTensor(state).to(device)

        if np.random.rand() <= self.epsilon:
            action = random.randrange(self.action_size)
        else:
            q_value = self.model(state)
            action = torch.argmax(q_value).item()

        return action
```

- `get_action`: ì…ì‹¤ë¡  íƒìš• ì •ì±…ì„ ì‚¬ìš©í•˜ì—¬ í–‰ë™ì„ ê²°ì •í•œë‹¤. íƒí—˜ì„ í†µí•´ ë¬´ì‘ìœ„ë¡œ í–‰ë™ì„ ê²°ì •í•˜ê±°ë‚˜, ì¸ê³µì‹ ê²½ë§ì„ í†µí•´ íƒìš•ì ìœ¼ë¡œ í–‰ë™ì„ ì„ íƒí•œë‹¤.

```python
    def append_sample(self, state, action, reward, next_state, done):
        state = state
        next_state = next_state
        self.memory.append((state, action, reward, next_state, done))
```

- `append_sample`: â€˜ìƒíƒœ-í–‰ë™-ë³´ìƒ-ë‹¤ìŒ ìƒíƒœ-ì™„ë£Œâ€™ì˜ ê°’ì„ ê°–ëŠ” ìƒ˜í”Œì„ ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì— ì €ì¥í•œë‹¤.

```python
    def train_model(self):
        if len(self.memory) < self.batch_size:
            return

        minibatch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*minibatch)

        states = np.array(states)
        actions = np.array(actions)
        rewards = np.array(rewards)
        next_states = np.array(next_states)
        dones = np.array(dones)

        states = states.reshape(self.batch_size, 1, self.grid_size_X, self.grid_size_Y)
        next_states = next_states.reshape(self.batch_size, 1, self.grid_size_X, self.grid_size_Y)

        states = torch.tensor(states, dtype=torch.float32).to(device)
        actions = torch.tensor(actions, dtype=torch.long).to(device)
        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)
        dones = torch.tensor(dones, dtype=torch.float32).to(device)

        pred = self.model(states
        target_pred = self.target_model(next_states).max(1)[0].detach()

        targets = rewards + (1 - dones) * self.discount_factor * target_pred

        pred = pred.gather(1, actions.unsqueeze(1))
        trg = targets.unsqueeze(1)

        loss = self.loss(pred, trg)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.scheduler.step()

        self.target_update_counter += 1
        if self.target_update_counter >= self.update_target_every:
            self.target_update_counter = 0
            self.update_target_model()
```

- `train_model`: ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì—ì„œ ìƒ˜í”Œë§í•œ ë°°ì¹˜ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  íƒ€ê¹ƒ ëª¨ë¸ì„ ì£¼ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•œë‹¤.
    - **ëª¨ë¸ í•™ìŠµ ê³¼ì •**
        - ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•˜ì—¬ ë°°ì¹˜ ìƒì„±
        - í˜„ì¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒíƒœì˜ Q-value ì˜ˆì¸¡
        - íƒ€ê¹ƒ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ìƒíƒœì˜ ìµœëŒ€ Q-valueì„ ê³„ì‚°í•˜ê³  ë²¨ë§Œ ìµœì  ë°©ì •ì‹ìœ¼ë¡œ íƒ€ê¹ƒ ì—…ë°ì´íŠ¸
        - ì˜ˆì¸¡í•œ Qê°’ê³¼ íƒ€ê¹ƒ Qê°’ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ loss êµ¬í•¨
        - lossë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ë° í•™ìŠµë¥  ì¡°ì •

```python
    def validate_model(self, validation_env, episodes=100):
        self.model.eval()
        total_score = 0
        total_wins = 0

        for epi in range(episodes):
            state = validation_env.reset()
            done = False
            score = 0

            while not done:
                action = self.get_action(state)
                next_state, reward, done = validation_env.step(action)
                score += reward
                state = next_state

            total_score += score
            if not validation_env.explode:
                total_wins += 1

        avg_score = total_score / episodes
        win_rate = (total_wins / episodes) * 100

        print(f"Validation results over {episodes} episodes:")
        print(f"Average score: {avg_score:.2f}")
        print(f"Win rate: {win_rate:.2f}%")

        self.model.train()
```

- `validate_model`: validation í™˜ê²½ì—ì„œ ì—ì´ì „íŠ¸ë¥¼ í‰ê°€í•˜ê³  í‰ê·  ì ìˆ˜ì™€ ìŠ¹ë¥ ì„ ì¶œë ¥í•œë‹¤.
***
# Train

```python
env = Environment()

state_size = env.state_size
action_size = env.state_size
grid_size_X = env.grid_size_X
grid_size_Y = env.grid_size_Y

agent = MineSweeper(state_size, action_size, grid_size_X, grid_size_Y, env)

EPISODES = 100000
RENDER_PROCESS = False
RENDER_END = False
total_moves = []

scores = np.zeros(EPISODES)
episodes = np.zeros(EPISODES)
length_memory = np.zeros(EPISODES)
wins = np.zeros(EPISODES)
timesteps = np.zeros(EPISODES)
rewards = []

N = 500
CHECKPOINT_INTERVAL = 5000

VALIDATION_INTERVAL_INITIAL = 1000
VALIDATION_INTERVAL_MIDDLE = 5000
VALIDATION_INTERVAL_LATE = 20000

validation_env = ValidationEnvironment()
```

- í•™ìŠµ ì¤‘ê°„ì¤‘ê°„ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ì²´í¬í•˜ë©°, ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•˜ê±°ë‚˜ ëª¨ë¸ì´ ê³¼ì í•©ë˜ì§€ ì•Šë„ë¡ ë°©ì§€í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” Validationì„ ì¶”ê°€í–ˆë‹¤.
- í•™ìŠµ ì‹œ ì´ˆê¸° ì—í”¼ì†Œë“œë“¤ì—ì„œëŠ” íƒí—˜ì„ í†µí•´ í•™ìŠµì´ ë¹ ë¥´ê²Œ ë³€í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë” ìì£¼ Validationì„ í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ë‹¤ê³  ìƒê°í•´ì„œ ì´ˆê¸°ì˜ Validation ì£¼ê¸°ë¥¼ 1000ìœ¼ë¡œ ì„¤ì •í–ˆë‹¤.

```python
for epi in range(EPISODES):
    done = False
    score = 0
    time_step = 0
    actions = []
    rewards = []

    state = env.reset()

    while not done and time_step <= 71:
        time_step += 1
        if env.first_move:
            mine_state = env.minefield.flatten()
            first_action = random.randint(0, len(mine_state)-1)
            first_state = mine_state[first_action]
            while first_state == -1:
                first_action = random.randint(0, len(mine_state)-1)
                first_state = mine_state[first_action]
            action = first_action
            env.first_move = False
        else:
            action = agent.get_action(state)

        next_state, reward, done = env.step(action)
        score += reward

        (action_x, action_y) = divmod(action, env.grid_size_X)
        actions.append((action_x, action_y))
        rewards.append(reward)

        scaled_state = (next_state - (-1)) / (8 - (-1))
        agent.append_sample(state, action, reward, scaled_state, done)

        train_interval = 5
        if len(agent.memory) >= agent.train_start and time_step % train_interval == 0:
            agent.train_model()

        state = next_state

    scores[epi] = score
    timesteps[epi] = time_step

    if env.explode or time_step >= 71:
        wins[epi] = 0
    else:
        wins[epi] = 1
        print(f"episode: {epi}")
        print(f"episode score: {score}")
        print(f"time step: {time_step}")
        print(f"epsilon: {agent.epsilon:.4f}")
        env.render()
        print(f"chosen_coordinate: {actions}")

    if agent.epsilon > agent.epsilon_min:
        agent.epsilon *= agent.epsilon_decay

    if epi % N == 0:
        scores_N = np.median(scores[max(0, epi-N+1):epi+1])  
        episodes[epi] = epi
        win_rate = np.mean(wins[max(0, epi-N+1):epi+1]) * 100
        length_memory[epi] = len(agent.memory)
        print(f"episode: {epi:3d} | time step: {time_step}")
        print(f"episode score: {score} | epsilon: {agent.epsilon:.4f}\n")
        print(f"<last {N} episode> score: {scores_N:.2f} | win rate: {win_rate:.2f}%\n")
        print(f"wins: {np.sum(wins[max(0, epi-N+1):epi+1])}\n")
        print(f"length of memory: {length_memory[epi]}\n")
        env.render()
        print(f"chosen_coordinate: {actions}")
        print(f"reward per time step: {rewards}")
        print("--------------------------------------------------")
        
		# Validation
    if epi < 20000:
        validation_interval = VALIDATION_INTERVAL_INITIAL
    elif epi < 60000:
        validation_interval = VALIDATION_INTERVAL_MIDDLE
    else:
        validation_interval = VALIDATION_INTERVAL_LATE

    if epi % validation_interval == 0:
        print(f"Performing validation at episode {epi}...")
        agent.validate_model(validation_env, episodes=100)
        print("Validation complete.")
        print("--------------------------------------------------")

    # ëª¨ë¸ ì €ì¥
    if epi % CHECKPOINT_INTERVAL == 0:
        checkpoint_path = f"checkpoint_{epi}.tar"
        save_checkpoint(agent, agent.optimizer, epi, score, checkpoint_path)
        print(f"Checkpoint saved at episode {epi} to {checkpoint_path}.")
        print("--------------------------------------------------")
```

- ë©ˆì¶¤ ì¡°ê±´ ì„¤ì •: ì§€ë¢°ì°¾ê¸° ê²Œì„ì—ì„œ ìŠ¹ë¦¬í•˜ë ¤ë©´ 81ê°œì˜ ì¢Œí‘œ ì¤‘ ì§€ë¢° 10ê°œë¥¼  ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì „ë¶€ ì—° ìƒíƒœê°€ ë˜ì–´ì•¼ í•˜ë¯€ë¡œ time stepì´ 71 (81-10)ì— ë„ë‹¬í•˜ë©´ ì—í”¼ì†Œë“œê°€ ì¢…ë£Œë˜ë„ë¡ í–ˆë‹¤.
- state ì •ê·œí™”: ì‹ ê²½ë§ì˜ ì…ë ¥ì´ ë˜ëŠ” stateë¥¼ $value - min\over {max - min}$ ê³µì‹ì„ ì´ìš©í•´ ì •ê·œí™”í•œ í›„ ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì— ìƒ˜í”Œë¡œ ì €ì¥í•˜ì˜€ë‹¤.
- **í™œìš© ì§€í‘œ**
    - Episode score: time step ë‹¹ ì–»ëŠ” rewardì˜ í•©
    - Last N episode score: í•™ìŠµì„ ì§„í–‰í•˜ë©° ë§ˆì§€ë§‰ N(=500)ê°œì˜ ì—í”¼ì†Œë“œë“¤ì˜ scoreì˜ ì¤‘ê°„ê°’
    - wins: ì—í”¼ì†Œë“œì˜ ìŠ¹ë¦¬ ì—¬ë¶€ë¥¼ ì €ì¥í•œ ë°°ì—´
    - win rate: Nê°œì˜ ì—í”¼ì†Œë“œ ì¤‘ ìŠ¹ë¦¬í•œ ë¹„ìœ¨
    - ìŠ¹ë¦¬ ì—¬ë¶€ í™•ì¸: ì—ì´ì „íŠ¸ê°€ ì—í”¼ì†Œë“œ ì¢…ë£Œê¹Œì§€ 71 time step ì´ë‚´ì— ì–´ë–¤ ì§€ë¢°ë„ ì„ íƒí•˜ì§€ ì•Šê³  ë‚˜ë¨¸ì§€ íƒ€ì¼ì€ ì „ë¶€ ì—´ì—ˆë‹¤ë©´, ë„˜íŒŒì´ ë°°ì—´ winsì— 1ì„, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0ì„ ë„£ëŠ”ë‹¤. ë§ˆì§€ë§‰ 500ê°œì˜ ì—í”¼ì†Œë“œë“¤ì— í•´ë‹¹í•˜ëŠ” winsì˜ ìš”ì†Œ ì¤‘ 1ì˜ ê°œìˆ˜ë¥¼ ìŠ¹ë¦¬í•œ íšŸìˆ˜ë¡œ í•˜ì—¬ ìŠ¹ë¦¬ íšŸìˆ˜ ë° ìŠ¹ë¥ ì„ í™•ì¸í•˜ì˜€ë‹¤.
- Validation
    - 20000ë²ˆì§¸ ì—í”¼ì†Œë“œ ì „ê¹Œì§€ëŠ” 1000ì„ ì£¼ê¸°ë¡œ,
    - 60000ë²ˆì§¸ ì—í”¼ì†Œë“œ ì „ê¹Œì§€ëŠ” 5000ì„ ì£¼ê¸°ë¡œ,
    - ì´í›„ì—ëŠ” 20000ì„ ì£¼ê¸°ë¡œ validation
- CheckPointë§ˆë‹¤ ëª¨ë¸ ì €ì¥

# ì‹œë„í•œ ë°©ë²•ë¡  ë¶„ì„ ì•„ì´ë””ì–´ ë° ê²°ê³¼

1. ë¨¼ì € **ê²Œì„íŒì„ 10ê°œë¡œ í•œì •**í•˜ì—¬ ì„±ëŠ¥(ìŠ¹ë¥ )ì„ ë†’ì´ëŠ” ê²ƒì„ ì‹œë„í•¨
    
    ë‹¨ìˆœí•œ êµ¬ì¡°ì˜ DNNì„ netìœ¼ë¡œ ì‚¬ìš©í•˜ê³  learning rate schedulerê°€ lambdaLRì¼ ë•ŒëŠ”  í•™ìŠµ ì‹œ ê±°ì˜ í•œ ë²ˆë„ ìŠ¹ë¦¬í•˜ì§€ ëª»í•˜ë‹¤ê°€, CNNê³¼ cyclicLRìœ¼ë¡œ ë³€ê²½í•˜ë‹ˆ 50000 ì—í”¼ì†Œë“œë¡œ í•™ìŠµ ì‹œ í‰ê·  ìŠ¹ë¥  3.6%ë¥¼ ì›ƒëŒì•˜ë‹¤.
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/1c873709-ed4b-4a75-ae8b-055a2c375a93/095f9517-9925-478d-83e9-5cbc891249d4/Untitled.png)
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/1c873709-ed4b-4a75-ae8b-055a2c375a93/0aaaf646-9883-4685-bd1f-552d37e9d06a/Untitled.png)
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/1c873709-ed4b-4a75-ae8b-055a2c375a93/eedd1f14-4404-4e1a-9d59-62c9e4ec364c/Untitled.png)
    
2. **state ì •ê·œí™”** :
    
    Netì˜ inputìœ¼ë¡œ ì´ìš©í•  stateë¥¼ 0~1 ì‚¬ì´ ê°’ë“¤ë¡œ ì •ê·œí™” í•˜ì˜€ë‹¤.
    
    - **ì •ê·œí™”ì˜ ëª©ì **
    
    ![ì¶œì²˜) https://hyen4110.tistory.com/m/20](https://prod-files-secure.s3.us-west-2.amazonaws.com/1c873709-ed4b-4a75-ae8b-055a2c375a93/41456955-2188-49cb-ba8e-649158c09337/Untitled.png)
    
    ì¶œì²˜) https://hyen4110.tistory.com/m/20
    
    - **state ì •ê·œí™”ë¥¼ ì‹œí–‰í•œ ìœ„ì¹˜**
        
        ì •ê·œí™”ë¥¼ stepì—ì„œ í•˜ë©´ ë‹¤ìŒ stepì˜ stateê°€ scaled_stateê°€ ë˜ëŠ” ë¬¸ì œ ë°œìƒ
        
        (stateê³¼ scaled_state ê°„ì˜ êµ¬ë¶„ ë¶ˆê°€)
        
        â‡’ env.step() ëŒ€ì‹  í•™ìŠµ ë£¨í”„ì— scaled_stateë¥¼ ì •ì˜
        
        ```python
        for epi in range(EPISODES):
        		...
            state = env.reset() # 2ì°¨ì› ë°°ì—´ state
        
            while not done and time_step <= 71:
                time_step += 1
                if env.first_move:
                    mine_state = env.minefield.flatten()
                    first_action = random.randint(0, len(mine_state)-1)
                    first_state = mine_state[first_action]
                    while first_state == -1:
                        first_action = random.randint(0, len(mine_state)-1)
                        first_state = mine_state[first_action]
                    action = first_action
                    env.first_move = False
                else:
                    action = agent.get_action(state)
                   
                next_state, reward, done = env.step(action)
        
                # state (ì‹ ê²½ë§ì˜ input) ì •ê·œí™”
                scaled_state = (next_state - (-1)) / (8 - (-1))
        
                agent.append_sample(state, action, reward, scaled_state, done)
        ```
        
3. **validation - ê³¼ì í•© ë¬¸ì œ**
    
    trainì— ë¹„í•´ testì˜ ì„±ëŠ¥ì´ í˜„ì €í•˜ê²Œ ë–¨ì–´ì§€ëŠ” ë¬¸ì œ ë°œìƒ
    
    ![Train](https://prod-files-secure.s3.us-west-2.amazonaws.com/1c873709-ed4b-4a75-ae8b-055a2c375a93/491c0216-73ac-4368-b2d0-db407e5bf0a8/Untitled.png)
    
    Train
    
    ![Test](https://prod-files-secure.s3.us-west-2.amazonaws.com/1c873709-ed4b-4a75-ae8b-055a2c375a93/2e14f2a6-2130-4512-afc9-b8350a1f4ee9/Untitled.png)
    
    Test
    
    â‡’ validation ì½”ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³ ì í•¨
    
    ```python
    class ValidationEnvironment(Environment):
        def __init__(self):
            super().__init__()
    ```
    
    ```python
    class MineSweeper(nn.Module):
    		...
    		def validate_model(self, validation_env, episodes=100):
    	      self.model.eval()
            total_score = 0
            total_wins = 0
    
            for epi in range(episodes):
                state = validation_env.reset()
                done = False
                score = 0
    
                while not done:
                    action = self.get_action(state)
                    next_state, reward, done = validation_env.step(action)
                    score += reward
                    state = next_state
    
                total_score += score
                if not validation_env.explode:
                    total_wins += 1
    
            avg_score = total_score / episodes
            win_rate = (total_wins / episodes) * 100
    
            print(f"Validation results over {episodes} episodes:")
            print(f"Average score: {avg_score:.2f}")
            print(f"Win rate: {win_rate:.2f}%")
    
            self.model.train()
    ```
    
    ```python
        if epi < 20000:
            validation_interval = VALIDATION_INTERVAL_INITIAL
        elif epi < 60000:
            validation_interval = VALIDATION_INTERVAL_MIDDLE
        else:
            validation_interval = VALIDATION_INTERVAL_LATE
    
        if epi % validation_interval == 0:
            print(f"Performing validation at episode {epi}...")
            agent.validate_model(validation_env, episodes=100)
            print("Validation complete.")
    ```
    
4. ì—ì´ì „íŠ¸ê°€ ê°™ì€ íƒ€ì¼ì„ ì„ íƒí•  ìˆ˜ ìˆê²Œ í•  ê²ƒì¸ê°€?
    - í•œ ì—í”¼ì†Œë“œ ë‚´ì—ì„œ í•œ ë²ˆ ì„ íƒí•œ íƒ€ì¼ì€ ì´í›„ì˜ time stepì—ì„œ ì„ íƒí•˜ì§€ ëª»í•˜ë„ë¡ í•˜ëŠ” ë°©ë²•: ì˜¤íˆë ¤ ì—ì´ì „íŠ¸ê°€ â€˜ì´ë¯¸ ì—° íƒ€ì¼ì„ ë°˜ë³µí•´ì„œ ì„ íƒí•˜ëŠ” ê²ƒì€ ì¢‹ì§€ ì•Šì€ í–‰ë™â€™ì„ì„ í•™ìŠµí•˜ëŠ” ê±¸ ë°©í•´í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•˜ê²Œ ë˜ì—ˆë‹¤.
    - í•˜ì§€ë§Œ ê°™ì€ íƒ€ì¼ì„ ê³„ì† ì„ íƒí•˜ë‹¤ ë³´ë©´ time stepì´ ì—„ì²­ë‚˜ê²Œ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìŒ
        
        â†’ ìµœëŒ€ time step 71(81-10)ìœ¼ë¡œ ì œí•œí–ˆë‹¤. (ë©ˆì¶¤ ì¡°ê±´ ì„¤ì •)
        
5. ì§€ë¢°ë¥¼ ì„ íƒí–ˆì„ ë•Œì™€ ë°©ë¬¸í–ˆë˜ ì¢Œí‘œë¥¼ ë˜ ì„ íƒí•  ë•Œì˜ ë³´ìƒì´ ë™ì¼í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , í•™ìŠµì´ ì§„í–‰ë¨ì— ë”°ë¼ ì§€ë¢°ëŠ” ê±°ì˜ ì„ íƒí•˜ì§€ ì•ŠëŠ”ë° ì´ë¯¸ ê°”ë˜ (ì•ˆì „í•˜ë‹¤ê³  íŒë‹¨í•˜ëŠ”) ì¢Œí‘œëŠ” ê³„ì† ë°©ë¬¸í•˜ëŠ” ë¬¸ì œ ë°œìƒ: 
    
    ì´ë¯¸ ë°©ë¬¸í•œ íƒ€ì¼ì„ ë‹¤ì‹œ ë°©ë¬¸í•  ë•Œë§ˆë‹¤ í˜ë„í‹°ë¥¼ ì ì§„ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ê°€ ë™ì¼í•œ íƒ€ì¼ì„ ë°˜ë³µí•´ì„œ ë°©ë¬¸í•˜ì§€ ì•Šë„ë¡ ì‹œë„í•´ë³´ì•˜ë‹¤.
    
    ```python
    self.visit_count = {}
    
    self.rewards = {'explode' : -1, 'nonprogress' : -1,'open_nonzero' : 0.1, 'open_zero' : 0.3, 'clear' : 1}
    
    if (x, y) in self.visit_count:  # ì„ íƒí•œ ì¢Œí‘œ (x,y)ê°€ ì´ë¯¸ ë°©ë¬¸ëœ ê²½ìš°
    		self.visit_count[(x, y)] += 1  # ë°©ë¬¸ íšŸìˆ˜ ì¦ê°€
    		reward = self.rewards['nonprogress'] * self.visit_count[(x, y)]
    ```
    
6. ë³´ìƒ ì„¤ì •
    1. ì§€ë¢° ì„ íƒ ì‹œ (---) / ì§€ë¢° ì•„ë‹Œ ì¢Œí‘œ ì„ íƒ ì‹œ (+) / ìŠ¹ë¦¬ ì‹œ (++)
    2. ì§€ë¢° ì„ íƒ ì‹œ (---) / 0ì„ ì„ íƒí•˜ì—¬ ë§ì€ ì¢Œí‘œê°€ ì—´ë ¸ì„ ì‹œ (++) / 0ì´ ì•„ë‹Œ ìˆ«ì ì¢Œí‘œ ì„ íƒ ì‹œ (+) / ìŠ¹ë¦¬ ì‹œ (+++)
    3. ì§€ë¢° ì„ íƒ ì‹œ (---) / ì´ë¯¸ ì—° ì¢Œí‘œ ì„ íƒ ì‹œ (---) / ìƒˆë¡œìš´ 0 ì„ íƒ ì‹œ (++) / ìƒˆë¡œìš´ 0ì´ ì•„ë‹Œ ìˆ«ì ì¢Œí‘œ ì„ íƒ ì‹œ (+) / ìŠ¹ë¦¬ ì‹œ (+++)
7. Optimizer: Adamê³¼ RMSprop
8. Net: DNNê³¼ CNN / ê¸°ë³¸ì ì¸ CNNê³¼ ResNetì„ ì°¸ê³ í•œ êµ¬ì¡°
9. Learning rate scheduler: lambdaLR â†’ cyclicLR (íŒ 10ê°œ) â†’ StepLR
10. ëª¨ë¸ ì €ì¥(ì¶”ë¡  / í•™ìŠµ ì¬ê°œë¥¼ ìœ„í•´ ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸(checkpoint) ì €ì¥í•˜ê¸° & ë¶ˆëŸ¬ì˜¤ê¸°**)**
    - ì²´í¬í¬ì¸íŠ¸ ì €ì¥í•˜ê¸°
        
        ```python
        if epi % CHECKPOINT_INTERVAL == 0:
            checkpoint_path = f"checkpoint_{epi}.tar"
            save_checkpoint(agent, agent.optimizer, epi, score, checkpoint_path)
            print(f"Checkpoint saved at episode {epi} to {checkpoint_path}.")
        ```
        
    - ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
        
        ```python
        checkpoint_path = 'checkpoint_5000.tar'  # ì˜ˆ) 5000ë²ˆì§¸ ì—í”¼ì†Œë“œ ì²´í¬í¬ì¸íŠ¸
        agent, optimizer, start_epoch, last_loss = load_checkpoint(agent, optimizer, checkpoint_path)
        
        print(f"Checkpoint loaded from {checkpoint_path}. Starting from epoch {start_epoch}.")
        ```
        
***
# ì½”ë“œ ì†ë„ ê°œì„ 

1. ë¦¬ìŠ¤íŠ¸ì™€ ë„˜íŒŒì´ ë°°ì—´
    
    `UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)
    states = torch.tensor(states, dtype=torch.float32).to(device)` 
    
    ```python
    # Agent í´ë˜ìŠ¤ì˜ train_model() ë©”ì„œë“œ
    
    minibatch = random.sample(self.memory, self.batch_size)
    states, actions, rewards, next_states, dones = zip(*minibatch)
    # ì—¬ê¸°ì„œ ê° ìš”ì†Œë“¤ ë¶ˆëŸ¬ì˜¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ì§€ë¯€ë¡œ
    
    # ë„˜íŒŒì´ ë°°ì—´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ -> ë‹¤ì°¨ì› ë„˜íŒŒì´ ë°°ì—´ë¡œ
    states = np.array(states)
    actions = np.array(actions)
    rewards = np.array(rewards)
    next_states = np.array(next_states)
    dones = np.array(dones)
    ```
    
2. BFS
    - **`auto_reveal_tiles` ë©”ì†Œë“œ êµ¬í˜„ ë¬¸ì œ**
        - ì¬ê·€ í˜¸ì¶œ ë°©ì‹ ëŒ€ì‹  ë°˜ë³µë¬¸ê³¼ íë¥¼ ì´ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½í–ˆë‹¤.
        - ê¸°ì¡´ ì½”ë“œì—ì„œëŠ” ì¬ê·€ í˜¸ì¶œ ë°©ì‹ ì‚¬ìš© ê²°ê³¼, ë„ˆë¬´ ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ëŠ” ë¬¸ì œ ë°œìƒ
        
        ```python
        def auto_reveal_tiles(self, x, y):
                visited = set()  # ì¤‘ë³µëœ ê°’ í—ˆìš© X
                
                def reveal(x, y):
                    if (x, y) in visited:
                        return
                    visited.add((x, y))
                    self.playerfield[x, y] = self.minefield[x, y]
        
                    # ì£¼ë³€ 8ê°œ íƒ€ì¼ í™•ì¸
                    if self.minefield[x, y] == 0:
                        for dx in [-1, 0, 1]:
                            for dy in [-1, 0, 1]:
                                nx, ny = x + dx, y + dy
                                # ì¸ë±ìŠ¤ê°€ ê²Œì„íŒ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
                                if self.check_boundary(nx, ny) and (nx, ny) not in visited:
                                    reveal(nx, ny)
                reveal(x, y)
                return self.playerfield
        ```
        
        - íì™€ ë°˜ë³µë¬¸ì„ ì´ìš©í•œ BFS(ë„ˆë¹„ìš°ì„ íƒìƒ‰) êµ¬ì¡°ë¡œ ë³€ê²½ â‡’ ì†ë„ ê°œì„ 
            
            ```python
                def auto_reveal_tiles(self, x, y):  # BFS
                    queue = deque([(x, y)])
                    self.visited = set()
            
                    while queue:
                        cx, cy = queue.popleft()
                        self.visited.add((cx, cy))  # (cx, cy) ë°©ë¬¸ í‘œì‹œ
                        self.playerfield[cx, cy] = self.minefield[cx, cy]  # (cx, cy) íƒ€ì¼ ì—´ê¸°
                        self.visit_count[(cx, cy)] = self.visit_count.get((cx, cy), 0) + 1  # ë°©ë¬¸ íšŸìˆ˜ ê¸°ë¡
            
                        # (cx, cy) ì£¼ë³€ 8ê°œ íƒ€ì¼ í™•ì¸, ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´ íì— insert
                        if self.minefield[cx, cy] == 0: 
                            for dx in [-1, 0, 1]:
                                for dy in [-1, 0, 1]:
                                    nx, ny = cx + dx, cy + dy
                                    # ì¸ë±ìŠ¤ê°€ ê²Œì„íŒ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
                                    if self.check_boundary(nx, ny) and (nx, ny) not in self.visited and (nx, ny) not in queue:  # nonvisited ì£¼ìœ„ íƒ€ì¼ íì— ì¶”ê°€
                                        queue.append((nx, ny))
            ```
                  
***
# ìµœê³  ì„±ëŠ¥ì´ ë‚˜ì˜¨ ë°©ë²•ë¡  (ëª¨ë¸)

- **Adam optimizerë¥¼ ì´ìš©í•œ ëª¨ë¸**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/1c873709-ed4b-4a75-ae8b-055a2c375a93/b665a0c8-4368-46fb-9d5c-ce4ea58fca86/Untitled.png)

- **RMSprop optimizerë¥¼ ì´ìš©í•œ ëª¨ë¸**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/1c873709-ed4b-4a75-ae8b-055a2c375a93/f457119c-a946-4bf8-a444-5fac91cb0cb0/Untitled.png)

***
# ë¬¸ì œ í•´ê²° ë° ê°œì„ í•œ ì 

- Trainì˜ #safe first click ë¶€ë¶„ ìˆ˜ì •
    - ê¸°ì¡´: ì²« ì¢Œí‘œë¶€í„° ëŒë©´ì„œ -1ì´ ì•„ë‹Œ ì²˜ìŒ ì¢Œí‘œ ì„ íƒ â†’ ì²˜ìŒ opení•˜ëŠ” ì¢Œí‘œê°€ í•­ìƒ ë¹„ìŠ·í•´ì§ (ì™¼ìª½ ìƒë‹¨ ë¶€ë¶„ì— ìœ„ì¹˜í•œ ì¢Œí‘œë“¤ë§Œ ì„ íƒí•˜ê²Œ ë¨)
    - ìˆ˜ì •: ì „ì²´ ì¢Œí‘œì—ì„œ ëœë¤ ì„ íƒ â†’ ì§€ë¢° ì„ íƒí–ˆìœ¼ë©´ ë‹¤ì‹œ ë‹¤ë¥¸ ì¢Œí‘œ ëœë¤ ì„ íƒ
    
    ```python
    while not done and time_step <= 71:
            time_step += 1
            if env.first_move:
                mine_state = env.minefield.flatten()
                first_action = random.randint(0, len(mine_state)-1)
                first_state = mine_state[first_action]
                while first_state == -1:
                    first_action = random.randint(0, len(mine_state)-1)
                    first_state = mine_state[first_action]
                action = first_action
                env.first_move = False
            else:
                action = agent.get_action(state)
    ```
    
- **score ì‚°ì • ë°©ì‹**
    - step ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ë¹„íš¨ìœ¨ì ì´ë¼ëŠ” ì  ê³ ë ¤
    
    ê° ì—í”¼ì†Œë“œì˜ ì „ì²´ ë³´ìƒë“¤ì˜ medianìœ¼ë¡œ ì„¤ì •í–ˆë”ë‹ˆ ëŒ€ë¶€ë¶„ ë™ì¼í•œ ê°’ì´ ë‚˜ì˜¤ë˜ ë¬¸ì œ
    
    â‡’ ê° ì—í”¼ì†Œë“œì˜ scoreì„ í•´ë‹¹ ì—í”¼ì†Œë“œì˜ ë³´ìƒ ì´í•©ìœ¼ë¡œ ìˆ˜ì •
    
- **ê°€ëŠ¥í•˜ë©´ ìƒìˆ˜ì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •**
    
     â‡’  ì½”ë“œì˜ ë…ë¦½ì„± í–¥ìƒ
    
- **íƒí—˜ ë¶€ì¡±**
    - epsilon_decay ê°’ì„ ëŠ˜ë¦¼â†’ epsilonì´ ë” ì²œì²œíˆ ê°ì†Œí•˜ë„ë¡
    - epsilon_min ê°’ì„ ì¤„ì—¬ ë‚˜ì¤‘ì—ëŠ” ì •ì±…ì— ë” ì˜ì¡´í•  ìˆ˜ ìˆë„ë¡
    - ì—¬ëŸ¬ ë°°ì¹˜ í¬ê¸° ì‹œë„
