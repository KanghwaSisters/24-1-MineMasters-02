# 24-1-MineMasters-02
[ 24-1 /  MineMasters / Team 02 ]  
ğŸ‘©â€ğŸ’» ì´ì •ì—°, ì†ì£¼í˜„
***
# ëª©ì°¨
[Environment](#Environment)

[Net](#Net)

[Agent](#Agent)

[Train](#Train)

[ì½”ë“œ ì†ë„ ê°œì„ ](#ì½”ë“œ-ì†ë„-ê°œì„ )

[ì‹œë„í•œ ë°©ë²•ë¡ , ì•„ì´ë””ì–´ ê²°ê³¼ ë° ë¶„ì„](#ì‹œë„í•œ-ë°©ë²•ë¡ -ì•„ì´ë””ì–´-ê²°ê³¼-ë°-ë¶„ì„)

[ìµœê³  ì„±ëŠ¥ì´ ë‚˜ì˜¨ ëª¨ë¸](#ìµœê³ -ì„±ëŠ¥ì´-ë‚˜ì˜¨-ëª¨ë¸)

[ë¬¸ì œ í•´ê²° ë° ê°œì„ í•œ ì ](#ë¬¸ì œ-í•´ê²°-ë°-ê°œì„ í•œ-ì )

***

 # Environment

## Attributes

- **`grid_size_X`** , **`grid_size_Y`** : ê²Œì„íŒì˜ ê°€ë¡œ, ì„¸ë¡œ í¬ê¸°
- **`num_mines`** : ì§€ë¢° ê°œìˆ˜
- **state**
    - **`state_size`**: ì§€ë¢°íŒ í¬ê¸°
    - **`minefield`**  : ì§€ë¢° ì°¾ê¸° ê²Œì„ ìƒí™©ì´ ì €ì¥ëœ ì• íŠ¸ë¦¬ë·°íŠ¸(ì •ë‹µì§€)
        
        - **type:** np.array
        - **êµ¬ì¡°:** `grid_size_X` x `grid_size_Y` í¬ê¸°ì˜ 2D NumPy ë°°ì—´
        - ì´ˆê¸°ê°’: ëª¨ë“  ì…€ì´ 0 â†’ ì´í›„ `reset()` ë‚´ `place_mines()`ì—ì„œ ì§€ë¢°ë¥¼ ì‹¬ëŠ”ë‹¤. (-1ë¡œ update)
    - **`playerfield`** : í”Œë ˆì´ì–´ê°€ ë³´ëŠ” ì§€ë¢°ë°­ ìƒíƒœ
        
        *-1(mine), 0, 1, 2, 3, 4, 5, 6, 7, 8, 9(hidden)* 
        
        0~8ì€ ì¸ì ‘í•œ ì§€ë¢° ê°œìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
        
        - **type:** np.array
        - **êµ¬ì¡°:** `grid_size_X` x `grid_size_Y` í¬ê¸°ì˜ 2D NumPy ë°°ì—´
        - **ì´ˆê¸°ê°’:** ëª¨ë“  ì…€ì´ 9
- **ê²Œì„ ì§„í–‰ ê´€ë ¨ ë³€ìˆ˜ë“¤**
    - **`explode`**: í”Œë ˆì´ì–´ê°€ ì§€ë¢°ë¥¼ ë°Ÿì•˜ëŠ”ì§€ ì—¬ë¶€
    - **`done`**  : ê²Œì„ì´ ëë‚¬ëŠ”ì§€ ì—¬ë¶€
    - **`first_move`**: í˜„ì¬ ì›€ì§ì„ì´ ì²« ë²ˆì§¸ ì›€ì§ì„ì¸ì§€ ì—¬ë¶€
- ì´ë•Œ **`action`** ì€ ì—ì´ì „íŠ¸ê°€ playerfieldì—ì„œ ì„ íƒí•œ ì¢Œí‘œë¡œ ì •ì˜í•œë‹¤.
- ë³´ìƒ **`rewards`**
    | explode | noprogress | guess | progress | clear |
    | --- | --- | --- | --- | --- |
    | -1 | -1 | 0.1 | 0.3 | 1 |
    - 'guess' : 0.1
        - ì£¼ë³€ì„ ë‘˜ëŸ¬ì‹¼ íƒ€ì¼ë“¤ì´ ì „ë¶€ hidden tileì¸ ê²½ìš° (ì•„ë¬´ëŸ° ì •ë³´ ì—†ì´ ì°ì—ˆë‹¤ê³  ê°„ì£¼í•¨)
    - 'progress' : 0.3
        - ì£¼ë³€ì„ ë‘˜ëŸ¬ì‹¼ íƒ€ì¼ ì¤‘ì— í•˜ë‚˜ë¼ë„ openëœ íƒ€ì¼ì´ ìˆëŠ” ê²½ìš°
        - ê°€ì¥ìë¦¬ íƒ€ì¼ì€ ì£¼ë³€ì„ ë‘˜ëŸ¬ì‹¼ íƒ€ì¼ì´ 5ê°œ / ê¼­ì§“ì  íƒ€ì¼ì€ 3ê°œ

## Methods

### 1. **`__init__`**
ê²Œì„íŒ í¬ê¸°, minefield í¬ê¸°, playerfield í¬ê¸°, state í¬ê¸°, í­ë°œ ì—¬ë¶€, done ì—¬ë¶€, ì²« ìŠ¤í… ì—¬ë¶€, ë°©ë¬¸ ì¢Œí‘œ set, reward ë°°ì—´ ë“±ì˜ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•œë‹¤.

```python
class Environment:
    def __init__(self):
        self.grid_size_X = 9
        self.grid_size_Y = 9
        self.num_mines = 10

        self.minefield = np.zeros((self.grid_size_X, self.grid_size_Y), dtype=int)

        self.playerfield = np.full((self.grid_size_X, self.grid_size_Y), 9, dtype=int)

        self.state_size = self.minefield.size

        self.explode = False
        self.done = False
        self.first_move = True
        self.visited = set()

        self.rewards = {'explode' : -1, 'noprogress' : -0.1,'progress' : 0.3, 'guess' : 0.1, 'clear' : 1}
```

### 2. **`reset`**
ì—í”¼ì†Œë“œë¥¼ ì´ˆê¸° ìƒíƒœë¡œ ë˜ëŒë¦¬ëŠ” ì—­í• 
ìƒˆë¡œìš´ ê²Œì„ì— í•„ìš”í•œ ì§€ë¢°ë¥¼ ë°°ì¹˜í•˜ê³ , ìƒˆë¡œìš´ playerfield ë¥¼ ì œê³µí•œë‹¤.
- `place_mines`  : ì§€ë¢° ê°œìˆ˜ë§Œí¼ ì„ì˜ì˜ ì¢Œí‘œì— ì§€ë¢°ë¥¼ ì‹¬ì€ í›„, ì§€ë¢°ê°€ ì—†ëŠ” ì¢Œí‘œì— ëŒ€í•´ì„œëŠ” ì¸ì ‘í•œ ì§€ë¢°ê°œìˆ˜ë¥¼ playerfieldì— updateí•œë‹¤.
- `count_adjacent_mines` : ì¸ì ‘í•œ ì§€ë¢° ê°œìˆ˜ë¥¼ ì„¸ëŠ” ë©”ì†Œë“œ

```python
    def reset(self):
        self.minefield = np.zeros((self.grid_size_X, self.grid_size_Y), dtype=int)
        self.playerfield = np.full((self.grid_size_X, self.grid_size_Y), 9, dtype=int)

        self.explode = False
        self.done = False
        self.first_move = True

        self.visited = set()

        self.place_mines()

        return list(self.playerfield)
```
```python
    def place_mines(self):
        mines_placed = 0

        # num_minesë§Œí¼ ì„ì˜ì˜ ì¢Œí‘œì— ì§€ë¢° ì‹¬ê¸°
        while mines_placed < self.num_mines:
            x = random.randint(0, self.grid_size_X - 1)
            y = random.randint(0, self.grid_size_Y - 1)

            if self.minefield[x, y] == 0:
                self.minefield[x, y] = -1
                mines_placed += 1

        # ì§€ë¢° ì—†ëŠ” ì¢Œí‘œ: ì¸ì ‘ ì§€ë¢° ê°œìˆ˜ ì„¸ê¸°
        for x in range(self.grid_size_X):
            for y in range(self.grid_size_Y):
                if self.minefield[x, y] == -1:
                    continue
                self.minefield[x, y] = self.count_adjacent_mines(x, y)

    def count_adjacent_mines(self, x, y):
        count = 0
        # (x,y) ì£¼ë³€ ì§€ë¢° ê°œìˆ˜
        for i in range(max(0, x - 1), min(self.grid_size_X, x + 2)):
            for j in range(max(0, y - 1), min(self.grid_size_Y, y + 2)):
                if (i, j) != (x, y) and self.minefield[i, j] == -1:
                    count += 1
        return count
```

### 3. **`step`**
ì—ì´ì „íŠ¸ê°€ í™˜ê²½ì—ì„œ actionì„ í•œ ë‹¨ê³„ ìˆ˜í–‰í•  ë•Œë§ˆë‹¤ í˜¸ì¶œ

- `next_state`, `reward`, `done` ë°˜í™˜
- ë™ì‘ ê³¼ì •
    1. 1ì°¨ì› ì¸ë±ìŠ¤ `action` ë¥¼ 2ì°¨ì› ì¢Œí‘œë¡œ ë³€í™˜í•œë‹¤.
        
        (minefield, playerfield ëª¨ë‘ 2ì°¨ì›ì´ê¸° ë•Œë¬¸ì—)
        
    2. ì„ íƒí•œ ì¢Œí‘œ (x,y)ê°€ ì§€ë¢°ì¸ì§€ ì—¬ë¶€ë¥¼ minefieldì—ì„œ í™•ì¸
        1. ì§€ë¢°(-1)ì¸ ê²½ìš°
            - `done`, `explode`, `reward` ë¡œ ê°ê° True, True, rewards['explode'] ë°˜í™˜
            - ê²Œì„ íŒ¨ë°°
        2. ì§€ë¢°(-1)ê°€ ì•„ë‹Œ ê²½ìš°
            1. ì„ íƒí•œ ì¢Œí‘œê°€ ì´ë¯¸ openëœ ì¢Œí‘œì¸ ê²½ìš°
                - `done`, `explode`, `reward` ë¡œ ê°ê° False, False, rewards['noprogress'] ë°˜í™˜
                - `next_state`ë¡œ `playerfield` ë°˜í™˜
            2. ì„ íƒí•œ ì¢Œí‘œê°€ ì²˜ìŒ openëœ ì¢Œí‘œì¸ ê²½ìš°
                1. `visited` ë°°ì—´ì— íƒ€ì¼ ì¶”ê°€
                2. íƒ€ì¼ open (playerfieldì˜ ì¢Œí‘œì— minefieldì˜ í•´ë‹¹ ì¢Œí‘œ ê°’ì„ ë³µì‚¬)
                    - ì¤‘ì‹¬ë¶€ íƒ€ì¼ì€ ì£¼ë³€ì„ ë‘˜ëŸ¬ì‹¼ íƒ€ì¼ì´ 8ê°œ / ê°€ì¥ìë¦¬ íƒ€ì¼ì€ 5ê°œ / ê¼­ì§“ì  íƒ€ì¼ì€ 3ê°œ
                        1. opení•œ íƒ€ì¼ì´ ì£¼ë³€ì´ ì „ë¶€ hiddenì¸ ê²½ìš°
                            - `reward`ë¡œ rewards['guess'] ë¶€ì—¬
                        2. opení•œ íƒ€ì¼ ì£¼ë³€ì— ì´ë¯¸ openëœ íƒ€ì¼ì´ ìˆëŠ” ê²½ìš°
                            - `reward`ë¡œ rewards['progress'] ë¶€ì—¬
                3. opení•œ íƒ€ì¼ì´ 0ì´ë©´ ì£¼ìœ„ íƒ€ì¼ open (`auto_reveal_tiles`)
                4. hidden tile(9)ì´ ë‚¨ì•„ìˆëŠ” ê²½ìš° `done=False`, ë‚¨ì•„ìˆì§€ ì•Šì€ ê²½ìš° `done=True` ë°˜í™˜
                5. `next_state`ë¡œ playerfield ë°˜í™˜
        ![step()_flowchart](https://github.com/user-attachments/assets/c80f349a-d6cb-4ca9-9df2-f28cd16763ef)

```python
def step(self, action):
        x, y = divmod(action, self.grid_size_X)

        reward = 0
        done = False

        # explode: ì§€ë¢° ì„ íƒ ì‹œ done
        if self.minefield[x, y] == -1:
            self.playerfield[x, y] = self.minefield[x, y]  # íƒ€ì¼ ì—´ê¸°
            self.explode = True
            done = True
            reward = self.rewards['explode']

        # ì§€ë¢°ë¥¼ ì„ íƒí•˜ì§€ ì•Šì€ ê²½ìš°
        else:
          # noprogress: ì„ íƒí•œ ì¢Œí‘œ (x,y)ê°€ ì´ë¯¸ ë°©ë¬¸ëœ ê²½ìš°
            if (x, y) in self.visited:
                reward = self.rewards['noprogress']
          # ì„ íƒí•œ ì¢Œí‘œ (x, y)ê°€ ì²˜ìŒ ë°©ë¬¸ëœ ê²½ìš°
            else:
                self.playerfield[x, y] = self.minefield[x, y]  # íƒ€ì¼ ì—´ê¸°
                self.visited.add((x,y))
                # ê°€ì¥ìë¦¬ íƒ€ì¼
                if x in [0, 8] or y in [0, 8]:
                    # guess
                    if self.count_adjacent_hidden(x, y) == 5:
                        reward = self.rewards['guess']
                    # progress
                    else:
                        reward = self.rewards['progress']
                # ê¼­ì§“ì  íƒ€ì¼
                elif x in [0, 8] and y in [0, 8]:
                    # guess
                    if self.count_adjacent_hidden(x, y) == 3:
                        reward = self.rewards['guess']
                    # progress
                    else:
                        reward = self.rewards['progress']
                else:
                    if self.count_adjacent_hidden(x, y) == 8:
                        reward = self.rewards['guess']
                    # progress
                    else:
                        reward = self.rewards['progress']
                # opení•œ íƒ€ì¼ì´ 0ì´ë©´ ì£¼ìœ„ íƒ€ì¼ open
                if self.playerfield[x, y] == 0:
                  self.auto_reveal_tiles(x, y)  # (x, y) ì£¼ë³€ íƒ€ì¼ ì—´ê¸°

            # clear: ëª¨ë“  hidden íƒ€ì¼ì´ ì§€ë¢°ë§Œ ë‚¨ì•„ ìˆëŠ” ê²½ìš° ìŠ¹ë¦¬
            if np.count_nonzero(self.playerfield == 9) == self.num_mines:
                done = True
                reward = self.rewards['clear']

        self.done = done
        next_state = self.playerfield
        return next_state, reward, done
```
- íƒ€ì¼ì„ opení•  ë•Œ í•„ìš”í•œ ë©”ì„œë“œ
    - `check_boundary` : opení•  íƒ€ì¼ì´ ê²Œì„íŒì„ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡
    - `auto_reveal_tiles`  : 0ì„ ì„ íƒí•œ ê²½ìš° ì—°ì‡„ì ìœ¼ë¡œ ì£¼ìœ„ íƒ€ì¼ì„ ì „ë¶€ open
        - BFS êµ¬ì¡°
        - ì£¼ë³€ 8ê°œ íƒ€ì¼ì˜ ìˆ«ì í™•ì¸, ê²Œì„íŒì„ ë²—ì–´ë‚˜ì§€ ì•Šê³  ë°©ë¬¸í•˜ì§€ ì•Šì€ ê²½ìš° íì— ì¶”ê°€
     
- ì„ íƒí•œ íƒ€ì¼ ì£¼ë³€ hidden tile ê°œìˆ˜ë¥¼ ì„¸ëŠ” ë°ì— í•„ìš”í•œ ë©”ì„œë“œ
    - `count_adjacent_hidden`

```python
    def check_boundary(self, x, y):
        return 0 <= x < self.grid_size_X and 0 <= y < self.grid_size_Y

    def auto_reveal_tiles(self, x, y):  # BFS
        queue = deque([(x, y)])

        while queue:
            cx, cy = queue.popleft()
            self.visited.add((cx, cy))  # (cx, cy) ë°©ë¬¸ í‘œì‹œ
            self.playerfield[cx, cy] = self.minefield[cx, cy]  # (cx, cy) íƒ€ì¼ ì—´ê¸°

            # (cx, cy) ì£¼ë³€ 8ê°œ íƒ€ì¼ í™•ì¸, ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´ íì— insert
            if self.minefield[cx, cy] == 0: # ë°©ë¬¸í•˜ì§€ ì•Šì•˜ìœ¼ë©´ open
                for dx in [-1, 0, 1]:
                    for dy in [-1, 0, 1]:
                        nx, ny = cx + dx, cy + dy
                        # ì¸ë±ìŠ¤ê°€ ê²Œì„íŒ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
                        if self.check_boundary(nx, ny) and (nx, ny) not in self.visited and (nx, ny) not in queue:  # nonvisited ì£¼ìœ„ íƒ€ì¼ íì— ì¶”ê°€
                            queue.append((nx, ny))
```
```python
    def count_adjacent_hidden(self, x, y):
        count = 0
        # (x,y) ì£¼ë³€ hidden tile ê°œìˆ˜
        for i in range(max(0, x - 1), min(self.grid_size_X, x + 2)):
            for j in range(max(0, y - 1), min(self.grid_size_Y, y + 2)):
                if (i, j) != (x, y) and self.playerfield[i, j] == 9:
                    count += 1
        return count
```

### 4. **`render`** 
íŠ¹ì • ì‹œì ì— playerfield ê²Œì„íŒì˜ ìƒíƒœë¥¼ render
- Hidden tile: **.**
- Mine: **X**
- ë‚˜ë¨¸ì§€: **0~8** (ì¸ì ‘í•œ ì§€ë¢° ìˆ˜)

```python
def render(self):  # ì¸ìˆ˜ ì„¤ì •
        for x in range(self.grid_size_X):
            for y in range(self.grid_size_Y):
                tile = self.playerfield[x, y]
                if tile == 9:
                    print('.', end=' ')
                elif tile == -1:
                    print('X', end=' ')
                else:
                    print(tile, end=' ')
                if y == self.grid_size_Y - 1:
                    print()
        print('\n')
```

***
# Net
```python
class Net(nn.Module):
    def __init__(self, action_size):
        super(Net, self).__init__()

        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=2, bias=False)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.conv4 = nn.Conv2d(64, action_size, kernel_size=3, stride=1, padding=1, bias=False)

        self.bn2 = nn.BatchNorm2d(64)
        self.bn3 = nn.BatchNorm2d(64)

        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)

        self.dropout = nn.Dropout(0.25)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = F.relu(self.bn3(self.conv3(x)))
        x = self.pool(F.relu(self.conv4(x)))
        x = self.dropout(x)
        x = torch.mean(x, dim=[2, 3])  # Global Average Pooling
        return x
```

- ë°°ì¹˜ ì •ê·œí™” (Batch normalization)
    - ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ” ì¥ì 
        - í•™ìŠµ ì†ë„ê°€ ë¹¨ë¼ì§
        - íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”ì— ëœ ë¯¼ê°í•¨
        - ëª¨ë¸ì„ ì¼ë°˜í™”
- í’€ë§ (Pooling)
    - Pooling layerëŠ” convolution layerì˜ ì¶œë ¥ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. ê·¸ë¦¬ê³  ì¶œë ¥ ë°ì´í„°ì˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ íŠ¹ì • ë°ì´í„°ë¥¼ ê°•ì¡°í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš©ëœë‹¤.
    - Netì˜ íŒŒë¼ë¯¸í„° ê°œìˆ˜ë‚˜ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ downsamplingí•˜ëŠ” ê²ƒì´ë‹¤.
    - Receptive fieldë¥¼ í¬ê²Œ ë§Œë“¤ì–´ ì „ì—­ì  íŠ¹ì§•ì„ ë” ì˜ í¬ì°©í•˜ë„ë¡ í•  ìˆ˜ ìˆë‹¤.

***
# Agent

### Hyperparameters

```python
DISCOUNT_FACTOR = 0.1
LEARNING_RATE = 0.01

EPSILON = 0.99
EPSILON_DECAY = 0.9999
EPSILON_MIN = 0.01

TARGET_UPDATE_COUNTER = 0
UPDATE_TARGET_EVERY = 5

BATCH_SIZE = 64
TRAIN_START = 1000
MAX_LEN = 50000
```

- **DISCOUNT_FACTOR**: ë¯¸ë˜ ë³´ìƒì˜ í˜„ì¬ ê°€ì¹˜ì— ëŒ€í•œ í• ì¸ ê³„ìˆ˜ (Gamma)
- **LEARNING_RATE**: Optimizerì— ì ìš©í•  í•™ìŠµë¥ 
- **EPSILON**: íƒí—˜ì„ í•  í™•ë¥ ì„ ì¡°ì ˆí•˜ëŠ” ê°’ (ì´ˆê¸° 0.99)
- **EPSILON_MIN**: (ìµœì†Œ 0.01)
- **EPSILON_DECAY** ë§¤ ì—í”¼ì†Œë“œê°€ ëë‚  ë•Œë§ˆë‹¤ ì…ì‹¤ë¡ ì— ê³±í•˜ì—¬ ì…ì‹¤ë¡  ê°’ì„ ì‘ì•„ì§€ê²Œ í•˜ëŠ” ê°’
- **BATCH_SIZE**: ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì—ì„œ ìƒ˜í”Œë§í•  ë°°ì¹˜ í¬ê¸°
- **TRAIN_START**: í•™ìŠµì„ ì‹œì‘í•  ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì˜ ìµœì†Œ í¬ê¸°
- **MAX_LEN**: ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì˜ ìµœëŒ€ ê¸¸ì´
- **TARGET_UPDATE_EVERY**: íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•  ì£¼ê¸°

### Methods

1. **`__init__`**

ìƒíƒœ í¬ê¸°, í–‰ë™ í¬ê¸°, í•˜ì´í¼íŒŒë¼ë¯¸í„°, ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬, ëª¨ë¸, íƒ€ê¹ƒ ëª¨ë¸ ë“±ì„ ì´ˆê¸°í™”í•œë‹¤.

```python
class MineSweeper(nn.Module):
    def __init__(self, state_size, action_size, grid_size_X, grid_size_Y, environment):
        super(MineSweeper, self).__init__()
        self.render = False

        self.state_size = state_size
        self.action_size = action_size
        self.grid_size_X = grid_size_X
        self.grid_size_Y = grid_size_Y

        self.environment = environment

        self.discount_factor = DISCOUNT_FACTOR
        self.learning_rate = LEARNING_RATE
        self.epsilon = EPSILON
        self.epsilon_decay = EPSILON_DECAY
        self.epsilon_min = EPSILON_MIN

        self.target_update_counter = TARGET_UPDATE_COUNTER
        self.update_target_every = UPDATE_TARGET_EVERY

        self.batch_size = BATCH_SIZE
        self.train_start = TRAIN_START
        self.maxlen = MAX_LEN
        self.minlen = MIN_LEN

        self.memory = deque(maxlen=self.maxlen)

        self.model = Net(self.action_size).to(device)
        self.target_model = Net(self.action_size).to(device)
        self.loss = nn.MSELoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        self.scheduler = optim.lr_scheduler.CyclicLR(optimizer=self.optimizer, base_lr=0.0001, max_lr=0.1, step_size_up=10000, mode='exp_range')

        self.update_target_model()
```
2. **`update_target_model`**
    
    íƒ€ê¹ƒ ëª¨ë¸ì„ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¡œ ì—…ë°ì´íŠ¸í•œë‹¤.
    

```python
    def update_target_model(self):
        self.target_model.load_state_dict(self.model.state_dict())
```

3. **`get_action`**
    
    ì…ì‹¤ë¡  íƒìš• ì •ì±…ì„ ì‚¬ìš©í•˜ì—¬ í–‰ë™ì„ ê²°ì •í•œë‹¤. íƒí—˜ì„ í†µí•´ ë¬´ì‘ìœ„ë¡œ í–‰ë™ì„ ê²°ì •í•˜ê±°ë‚˜, ì¸ê³µ ì‹ ê²½ë§ì„ í†µí•´ íƒìš•ì ìœ¼ë¡œ í–‰ë™ì„ ì„ íƒí•œë‹¤.
    
    - `action = torch.argmax(q_value).item()`
        
        ë°˜í™˜ê°’ : `q_value` í…ì„œì—ì„œ ìµœëŒ€ê°’ì„ ê°€ì§€ëŠ” ìš”ì†Œ (ì—¬ëŸ¬ ê°œì—¬ë„ ì²« ë²ˆì§¸ í•˜ë‚˜ì˜ ìš”ì†Œ(`torch.argmax()`)ì˜ ì¸ë±ìŠ¤ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜ ê°’(`.item()`)
        

```python
    def get_action(self, state):
        state = np.array(state).reshape(1, 1, self.grid_size_X, self.grid_size_Y)
        state = torch.FloatTensor(state).to(device)

        if np.random.rand() <= self.epsilon:
            action = random.randrange(self.action_size)
        else:
            q_value = self.model(state)
            action = torch.argmax(q_value).item()

        return action
```

4. **`append_sample`**
    
    ìƒ˜í”Œì„ ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì— ì €ì¥í•œë‹¤.
    

```python
    def append_sample(self, state, action, reward, next_state, done):
        state = state
        next_state = next_state
        self.memory.append((state, action, reward, next_state, done))
```

5. **`train_model`**
    
    ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì—ì„œ ìƒ˜í”Œë§í•œ ë°°ì¹˜ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  íƒ€ê¹ƒ ëª¨ë¸ì„ ì£¼ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•œë‹¤.
    
    - **ëª¨ë¸ í•™ìŠµ ê³¼ì •**
        - ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•˜ì—¬ ë°°ì¹˜ ìƒì„±
        - í˜„ì¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒíƒœì˜ Q-value ì˜ˆì¸¡
        - íƒ€ê¹ƒ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ìƒíƒœì˜ ìµœëŒ€ Q-valueì„ ê³„ì‚°í•˜ê³  ë²¨ë§Œ ìµœì  ë°©ì •ì‹ìœ¼ë¡œ íƒ€ê¹ƒ ì—…ë°ì´íŠ¸
        - ì˜ˆì¸¡í•œ Qê°’ê³¼ íƒ€ê¹ƒ Qê°’ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ loss êµ¬í•¨
        - lossë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ë° í•™ìŠµë¥  ì¡°ì •

```python
   	def train_model(self):
        if len(self.memory) < self.batch_size:
            return
        minibatch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*minibatch)

        states = np.array(states)
        actions = np.array(actions)
        rewards = np.array(rewards)
        next_states = np.array(next_states)
        dones = np.array(dones)

        states = states.reshape(self.batch_size, 1, self.grid_size_X, self.grid_size_Y)
        next_states = next_states.reshape(self.batch_size, 1, self.grid_size_X, self.grid_size_Y)

        states = torch.tensor(states, dtype=torch.float32).to(device)
        actions = torch.tensor(actions, dtype=torch.long).to(device)
        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)
        dones = torch.tensor(dones, dtype=torch.float32).to(device)

        pred = self.model(states)
        target_pred = self.target_model(next_states).max(1)[0].detach()

        targets = rewards + (1 - dones) * self.discount_factor * target_pred

        pred = pred.gather(1, actions.unsqueeze(1))
        trg = targets.unsqueeze(1)

        loss = self.loss(pred, trg)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        self.scheduler.step()

        self.target_update_counter += 1
        if self.target_update_counter >= self.update_target_every:
            self.target_update_counter = 0
            self.update_target_model()
```

***
# Train

- **ë©ˆì¶¤ ì¡°ê±´ ì„¤ì •**: ì§€ë¢°ì°¾ê¸° ê²Œì„ì—ì„œ ìŠ¹ë¦¬í•˜ë ¤ë©´ 81ê°œì˜ ì¢Œí‘œ ì¤‘ ì§€ë¢° 10ê°œë¥¼  ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì „ë¶€ ì—° ìƒíƒœê°€ ë˜ì–´ì•¼ í•˜ë¯€ë¡œ time stepì´ 71 (81-10)ì— ë„ë‹¬í•˜ë©´ ì—í”¼ì†Œë“œê°€ ì¢…ë£Œë˜ë„ë¡ í–ˆë‹¤.
- **state ì •ê·œí™”**: ì‹ ê²½ë§ì˜ ì…ë ¥ì´ ë˜ëŠ” stateë¥¼ $value - min\over {max - min}$ ê³µì‹ì„ ì´ìš©í•´ ì •ê·œí™”í•œ í›„ ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì— ìƒ˜í”Œë¡œ ì €ì¥í•˜ì˜€ë‹¤.
- **í™œìš© ì§€í‘œ**
    - Episode score: time step ë‹¹ ì–»ëŠ” rewardì˜ í•©
    - Last N episode score: í•™ìŠµì„ ì§„í–‰í•˜ë©° ë§ˆì§€ë§‰ N(=500)ê°œì˜ ì—í”¼ì†Œë“œë“¤ì˜ scoreì˜ ì¤‘ê°„ê°’
    - wins: ì—í”¼ì†Œë“œì˜ ìŠ¹ë¦¬ ì—¬ë¶€ë¥¼ ì €ì¥í•œ ë°°ì—´
    - win rate: Nê°œì˜ ì—í”¼ì†Œë“œ ì¤‘ ìŠ¹ë¦¬í•œ ë¹„ìœ¨
    - ìŠ¹ë¦¬ ì—¬ë¶€ í™•ì¸: ì—ì´ì „íŠ¸ê°€ ì—í”¼ì†Œë“œ ì¢…ë£Œê¹Œì§€ 71 time step ì´ë‚´ì— ì–´ë–¤ ì§€ë¢°ë„ ì„ íƒí•˜ì§€ ì•Šê³  ë‚˜ë¨¸ì§€ íƒ€ì¼ì€ ì „ë¶€ ì—´ì—ˆë‹¤ë©´, ë„˜íŒŒì´ ë°°ì—´ winsì— 1ì„, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0ì„ ë„£ëŠ”ë‹¤. ë§ˆì§€ë§‰ 500ê°œì˜ ì—í”¼ì†Œë“œë“¤ì— í•´ë‹¹í•˜ëŠ” winsì˜ ìš”ì†Œ ì¤‘ 1ì˜ ê°œìˆ˜ë¥¼ ìŠ¹ë¦¬í•œ íšŸìˆ˜ë¡œ í•˜ì—¬ ìŠ¹ë¦¬ íšŸìˆ˜ ë° ìŠ¹ë¥ ì„ í™•ì¸í•˜ì˜€ë‹¤.
- CheckPointë§ˆë‹¤ ëª¨ë¸ì„ ì €ì¥í–ˆë‹¤.
  
***
# ì½”ë“œ ì†ë„ ê°œì„ 

1. ë¦¬ìŠ¤íŠ¸ì™€ ë„˜íŒŒì´ ë°°ì—´
    
    `UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)
    states = torch.tensor(states, dtype=torch.float32).to(device)` 
    
    ```python
    # Agent í´ë˜ìŠ¤ì˜ train_model() ë©”ì„œë“œ
    
    minibatch = random.sample(self.memory, self.batch_size)
    states, actions, rewards, next_states, dones = zip(*minibatch)
    # ì—¬ê¸°ì„œ ê° ìš”ì†Œë“¤ ë¶ˆëŸ¬ì˜¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ì§€ë¯€ë¡œ
    
    # ë„˜íŒŒì´ ë°°ì—´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ -> ë‹¤ì°¨ì› ë„˜íŒŒì´ ë°°ì—´ë¡œ
    states = np.array(states)
    actions = np.array(actions)
    rewards = np.array(rewards)
    next_states = np.array(next_states)
    dones = np.array(dones)
    ```
    
2. BFS
    - **`auto_reveal_tiles` ë©”ì„œë“œ êµ¬í˜„ ë¬¸ì œ**
        - ì¬ê·€ í˜¸ì¶œ ë°©ì‹ ëŒ€ì‹  ë°˜ë³µë¬¸ê³¼ íë¥¼ ì´ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½í–ˆë‹¤.
        - ê¸°ì¡´ ì½”ë“œì—ì„œëŠ” ì¬ê·€ í˜¸ì¶œ ë°©ì‹ ì‚¬ìš© ê²°ê³¼, ë„ˆë¬´ ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ëŠ” ë¬¸ì œ ë°œìƒ
        
            ```python
            def auto_reveal_tiles(self, x, y):
                    visited = set()  # ì¤‘ë³µëœ ê°’ í—ˆìš© X
                    
                    def reveal(x, y):
                        if (x, y) in visited:
                            return
                        visited.add((x, y))
                        self.playerfield[x, y] = self.minefield[x, y]
            
                        # ì£¼ë³€ 8ê°œ íƒ€ì¼ í™•ì¸
                        if self.minefield[x, y] == 0:
                            for dx in [-1, 0, 1]:
                                for dy in [-1, 0, 1]:
                                    nx, ny = x + dx, y + dy
                                    # ì¸ë±ìŠ¤ê°€ ê²Œì„íŒ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
                                    if self.check_boundary(nx, ny) and (nx, ny) not in visited:
                                        reveal(nx, ny)
                    reveal(x, y)
                    return self.playerfield
            ```
        
        - íì™€ ë°˜ë³µë¬¸ì„ ì´ìš©í•œ BFS(ë„ˆë¹„ìš°ì„ íƒìƒ‰) êµ¬ì¡°ë¡œ ë³€ê²½ â‡’ ì†ë„ ê°œì„ 
            
            ```python
                def auto_reveal_tiles(self, x, y):  # BFS
                    queue = deque([(x, y)])
                    self.visited = set()
            
                    while queue:
                        cx, cy = queue.popleft()
                        self.visited.add((cx, cy))  # (cx, cy) ë°©ë¬¸ í‘œì‹œ
                        self.playerfield[cx, cy] = self.minefield[cx, cy]  # (cx, cy) íƒ€ì¼ ì—´ê¸°
                        self.visit_count[(cx, cy)] = self.visit_count.get((cx, cy), 0) + 1  # ë°©ë¬¸ íšŸìˆ˜ ê¸°ë¡
            
                        # (cx, cy) ì£¼ë³€ 8ê°œ íƒ€ì¼ í™•ì¸, ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´ íì— insert
                        if self.minefield[cx, cy] == 0: 
                            for dx in [-1, 0, 1]:
                                for dy in [-1, 0, 1]:
                                    nx, ny = cx + dx, cy + dy
                                    # ì¸ë±ìŠ¤ê°€ ê²Œì„íŒ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
                                    if self.check_boundary(nx, ny) and (nx, ny) not in self.visited and (nx, ny) not in queue:  # nonvisited ì£¼ìœ„ íƒ€ì¼ íì— ì¶”ê°€
                                        queue.append((nx, ny))
            ```
            
***
# ì‹œë„í•œ ë°©ë²•ë¡ , ì•„ì´ë””ì–´ ê²°ê³¼ ë° ë¶„ì„

1. ë¨¼ì € **ê²Œì„íŒì„ 10ê°œë¡œ í•œì •**í•˜ì—¬ ì„±ëŠ¥(ìŠ¹ë¥ )ì„ ë†’ì´ëŠ” ê²ƒì„ ì‹œë„í•¨
    
    ë‹¨ìˆœí•œ êµ¬ì¡°ì˜ DNNì„ netìœ¼ë¡œ ì‚¬ìš©í•˜ê³  learning rate schedulerê°€ lambdaLRì¼ ë•ŒëŠ”  í•™ìŠµ ì‹œ ê±°ì˜ í•œ ë²ˆë„ ìŠ¹ë¦¬í•˜ì§€ ëª»í•˜ë‹¤ê°€, CNNê³¼ cyclicLRìœ¼ë¡œ ë³€ê²½í•˜ë‹ˆ 50000 ì—í”¼ì†Œë“œë¡œ í•™ìŠµ ì‹œ í‰ê·  ìŠ¹ë¥  3.6%ë¥¼ ì›ƒëŒì•˜ë‹¤.
    
    ![10-highest_win_rate](https://github.com/user-attachments/assets/449a6fd3-38a6-48f5-8b45-48cb35f0f523)
    
    ![10-timesteps](https://github.com/user-attachments/assets/60eab1f8-aa81-4cff-967e-530639f6b268)
   
    ![10-score](https://github.com/user-attachments/assets/22b6c5eb-fdb3-468b-8306-d7463eb8d704)

   (rewards = {'explode' : -20, 'open_nonzero' : 5, 'open_zero' : 10, 'clear' : 20})
   
3. **state ì •ê·œí™”**
    
    Netì˜ inputìœ¼ë¡œ ì´ìš©í•  stateë¥¼ ì •ê·œí™”í•˜ì˜€ë‹¤.
    
    - **ì •ê·œí™”ì˜ ëª©ì ** : ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë° í­íŒŒ ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ inputì˜ ì ˆëŒ“ê°’ì„ íŠ¹ì • ë²”ìœ„ë¡œ ì œí•œí•˜ê¸° ë•Œë¬¸ì— ê·¸ë˜ë””ì–¸íŠ¸ê°€ saturateë˜ì§€ ì•Šë„ë¡ í•˜ì—¬ ë” ë¹ ë¥¸ ìµœì í™”ê°€ ê°€ëŠ¥í•´ì§„ë‹¤.
    - **state ì •ê·œí™”ë¥¼ ì‹œí–‰í•œ ìœ„ì¹˜**
        
        ì •ê·œí™”ë¥¼ stepì—ì„œ í•˜ë©´ ë‹¤ìŒ stepì˜ stateê°€ scaled_stateê°€ ë˜ëŠ” ë¬¸ì œ ë°œìƒ
        
        (stateê³¼ scaled_state ê°„ì˜ êµ¬ë¶„ ë¶ˆê°€)
        
        â‡’ env.step() ëŒ€ì‹  í•™ìŠµ ë£¨í”„ì— scaled_stateë¥¼ ì •ì˜
        
        ```python
        for epi in range(EPISODES):
        		...
            state = env.reset() # 2ì°¨ì› ë°°ì—´ state
        
            while not done and time_step <= 71:
                time_step += 1
                if env.first_move:
                    mine_state = env.minefield.flatten()
                    first_action = random.randint(0, len(mine_state)-1)
                    first_state = mine_state[first_action]
                    while first_state == -1:
                        first_action = random.randint(0, len(mine_state)-1)
                        first_state = mine_state[first_action]
                    action = first_action
                    env.first_move = False
                else:
                    action = agent.get_action(state)
                   
                next_state, reward, done = env.step(action)
        
                # state (ì‹ ê²½ë§ì˜ input) ì •ê·œí™”
                scaled_state = (next_state - (-1)) / (8 - (-1))
        
                agent.append_sample(state, action, reward, scaled_state, done)
        ```
        
4. **validation - ê³¼ì í•© ë¬¸ì œ**
    
    trainì— ë¹„í•´ testì˜ ì„±ëŠ¥ì´ í˜„ì €í•˜ê²Œ ë–¨ì–´ì§€ëŠ” ë¬¸ì œ ë°œìƒ
    
    ![valid-train](https://github.com/user-attachments/assets/357af465-3b75-4941-8c6f-2e50628fb58f)
    Train
    
    ![valid-test](https://github.com/user-attachments/assets/e56a3a9f-a19d-4899-a713-9e950864f4a6)
    Test
    
    â‡’ validation ì½”ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³ ì í•˜ì˜€ë‹¤.
    
    - validation í™˜ê²½ì—ì„œ ì—ì´ì „íŠ¸ë¥¼ í‰ê°€í•˜ê³  í‰ê·  ì ìˆ˜ì™€ ìŠ¹ë¥ ì„ ì¶œë ¥í•œë‹¤.
    - í•™ìŠµ ì¤‘ê°„ì¤‘ê°„ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ì²´í¬í•˜ë©°, ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•˜ê±°ë‚˜ ëª¨ë¸ì´ ê³¼ì í•©ë˜ì§€ ì•Šë„ë¡ ë°©ì§€í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” Validationì„ ì¶”ê°€í–ˆë‹¤.
    - í•™ìŠµ ì‹œ ì´ˆê¸° ì—í”¼ì†Œë“œë“¤ì—ì„œëŠ” íƒí—˜ì„ í†µí•´ í•™ìŠµì´ ë¹ ë¥´ê²Œ ë³€í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë” ìì£¼ Validationì„ í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ë‹¤ê³  ìƒê°í•´ì„œ ì´ˆê¸°ì˜ Validation ì£¼ê¸°ë¥¼ 1000ìœ¼ë¡œ ì„¤ì •í–ˆë‹¤.
    - Validation
        - 20000ë²ˆì§¸ ì—í”¼ì†Œë“œ ì „ê¹Œì§€ëŠ” 1000ì„ ì£¼ê¸°ë¡œ,
        - 60000ë²ˆì§¸ ì—í”¼ì†Œë“œ ì „ê¹Œì§€ëŠ” 5000ì„ ì£¼ê¸°ë¡œ,
        - ì´í›„ì—ëŠ” 20000ì„ ì£¼ê¸°ë¡œ validation
    
    ```python
    class ValidationEnvironment(Environment):
        def __init__(self):
            super().__init__()
    ```
    
    ```python
    class MineSweeper(nn.Module):
    		...
    		def validate_model(self, validation_env, episodes=100):
    	      self.model.eval()
            total_score = 0
            total_wins = 0
    
            for epi in range(episodes):
                state = validation_env.reset()
                done = False
                score = 0
    
                while not done:
                    action = self.get_action(state)
                    next_state, reward, done = validation_env.step(action)
                    score += reward
                    state = next_state
    
                total_score += score
                if not validation_env.explode:
                    total_wins += 1
    
            avg_score = total_score / episodes
            win_rate = (total_wins / episodes) * 100
    
            print(f"Validation results over {episodes} episodes:")
            print(f"Average score: {avg_score:.2f}")
            print(f"Win rate: {win_rate:.2f}%")
    
            self.model.train()
    ```
    
    ```python
    VALIDATION_INTERVAL_INITIAL = 1000
    VALIDATION_INTERVAL_MIDDLE = 5000
    VALIDATION_INTERVAL_LATE = 20000
    
    validation_env = ValidationEnvironment()
    
    ...
        
        if epi < 20000:
            validation_interval = VALIDATION_INTERVAL_INITIAL
        elif epi < 60000:
            validation_interval = VALIDATION_INTERVAL_MIDDLE
        else:
            validation_interval = VALIDATION_INTERVAL_LATE
    
        if epi % validation_interval == 0:
            print(f"Performing validation at episode {epi}...")
            agent.validate_model(validation_env, episodes=100)
            print("Validation complete.")
    ```
    
    - 10ë§Œ ë²ˆ~20ë§Œ ë²ˆì˜ ì—í”¼ì†Œë“œë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë©´ì„œ ìµœëŒ€í•œ ë¹„ìš©(Cost)ë¥¼ ì ˆê°í•˜ê¸° ìœ„í•´ ìµœì¢… ëª¨ë¸ì—ì„œëŠ” validationì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤.
5. ì—ì´ì „íŠ¸ê°€ **ê°™ì€ íƒ€ì¼ì„ ì„ íƒ**í•  ìˆ˜ ìˆê²Œ í•  ê²ƒì¸ê°€?
    - í•œ ì—í”¼ì†Œë“œ ë‚´ì—ì„œ í•œ ë²ˆ ì„ íƒí•œ íƒ€ì¼ì€ ì´í›„ì˜ time stepì—ì„œ ì„ íƒí•˜ì§€ ëª»í•˜ë„ë¡ í•˜ëŠ” ë°©ë²•: ì˜¤íˆë ¤ ì—ì´ì „íŠ¸ê°€ â€˜ì´ë¯¸ ì—° íƒ€ì¼ì„ ë°˜ë³µí•´ì„œ ì„ íƒí•˜ëŠ” ê²ƒì€ ì¢‹ì§€ ì•Šì€ í–‰ë™â€™ì„ì„ í•™ìŠµí•˜ëŠ” ê±¸ ë°©í•´í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•˜ê²Œ ë˜ì—ˆë‹¤.
    - í•˜ì§€ë§Œ ê°™ì€ íƒ€ì¼ì„ ê³„ì† ì„ íƒí•˜ë‹¤ ë³´ë©´ time stepì´ ì—„ì²­ë‚˜ê²Œ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆë‹¤.
        
        â†’ ìµœëŒ€ time step 71(81-10)ìœ¼ë¡œ ì œí•œí–ˆë‹¤. (ë©ˆì¶¤ ì¡°ê±´ ì„¤ì •)
        
6. ì§€ë¢°ë¥¼ ì„ íƒí–ˆì„ ë•Œì™€ ë°©ë¬¸í–ˆë˜ ì¢Œí‘œë¥¼ ë˜ ì„ íƒí•  ë•Œì˜ ë³´ìƒì´ ë™ì¼í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , í•™ìŠµì´ ì§„í–‰ë¨ì— ë”°ë¼ ì§€ë¢°ëŠ” ê±°ì˜ ì„ íƒí•˜ì§€ ì•ŠëŠ”ë° ì´ë¯¸ ê°”ë˜ (ì•ˆì „í•˜ë‹¤ê³  íŒë‹¨í•˜ëŠ”) ì¢Œí‘œëŠ” ê³„ì† ë°©ë¬¸í•˜ëŠ” ë¬¸ì œ ë°œìƒ: 
    
    ì´ë¯¸ ë°©ë¬¸í•œ íƒ€ì¼ì„ ë‹¤ì‹œ ë°©ë¬¸í•  ë•Œë§ˆë‹¤ í˜ë„í‹°ë¥¼ ì ì§„ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ê°€ ë™ì¼í•œ íƒ€ì¼ì„ ë°˜ë³µí•´ì„œ ë°©ë¬¸í•˜ì§€ ì•Šë„ë¡ ì‹œë„í•´ë³´ì•˜ë‹¤.
    
    ```python
    self.visit_count = {}
    
    self.rewards = {'explode' : -1, 'nonprogress' : -1,'open_nonzero' : 0.1, 'open_zero' : 0.3, 'clear' : 1}
    
    if (x, y) in self.visit_count:  # ì„ íƒí•œ ì¢Œí‘œ (x,y)ê°€ ì´ë¯¸ ë°©ë¬¸ëœ ê²½ìš°
    		self.visit_count[(x, y)] += 1  # ë°©ë¬¸ íšŸìˆ˜ ì¦ê°€
    		reward = self.rewards['nonprogress'] * self.visit_count[(x, y)]
    ```
    
    ë°©ë¬¸ íƒ€ì¼ ë³´ìƒ í˜ë„í‹° ëˆ„ì  ì—¬ë¶€ì— ë”°ë¼ ì„±ëŠ¥ì´ í¬ê²Œ ë‹¬ë¼ì§€ì§€ ì•Šì•˜ë‹¤.
    
7. ë³´ìƒ ì„¤ì •
    1. ì§€ë¢° ì„ íƒ ì‹œ (---) / ì§€ë¢° ì•„ë‹Œ ì¢Œí‘œ ì„ íƒ ì‹œ (+) / ìŠ¹ë¦¬ ì‹œ (++)
    2. ì§€ë¢° ì„ íƒ ì‹œ (---) / 0ì„ ì„ íƒí•˜ì—¬ ë§ì€ ì¢Œí‘œê°€ ì—´ë ¸ì„ ì‹œ (++) / 0ì´ ì•„ë‹Œ ìˆ«ì ì¢Œí‘œ ì„ íƒ ì‹œ (+) / ìŠ¹ë¦¬ ì‹œ (+++)
    3. ì§€ë¢° ì„ íƒ ì‹œ (---) / ì´ë¯¸ ì—° ì¢Œí‘œ ì„ íƒ ì‹œ (---) / ìƒˆë¡œìš´ 0 ì„ íƒ ì‹œ (++) / ìƒˆë¡œìš´ 0ì´ ì•„ë‹Œ ìˆ«ì ì¢Œí‘œ ì„ íƒ ì‹œ (+) / ìŠ¹ë¦¬ ì‹œ (+++)
    
    ìµœì¢… ëª¨ë¸ì—ëŠ” c.ì˜ ë³´ìƒ ì„¤ì • ë°©ì‹ì„ ì„ ì •í–ˆë‹¤.
    
8. Optimizer: Adamê³¼ RMSprop
9. Net: DNNê³¼ CNN / ê¸°ë³¸ì ì¸ CNNê³¼ ResNetì„ ì°¸ê³ í•œ êµ¬ì¡°
    
    ```python
    class ResidualBlock(nn.Module):
        def __init__(self, in_channels, out_channels, stride=1):
            super(ResidualBlock, self).__init__()
            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
            self.bn1 = nn.BatchNorm2d(out_channels)
            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
            self.bn2 = nn.BatchNorm2d(out_channels)
            self.downsample = nn.Sequential()
            if stride != 1 or in_channels != out_channels:
                self.downsample = nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                    nn.BatchNorm2d(out_channels)
                )
    
        def forward(self, x):
            residual = self.downsample(x)
            out = F.relu(self.bn1(self.conv1(x)))
            out = self.bn2(self.conv2(out))
            out += residual
            out = F.relu(out)
            return out
    ```
    
    ![residual_learning](https://github.com/user-attachments/assets/8b835ecd-78a6-4e56-9b45-92c0a2f6c017)
    
    - 2015ë…„ì— ê°œìµœëœÂ ILSVRC(ImageNet Large Scale Visual Recognition Challenge)ì—ì„œ ìš°ìŠ¹ì„ ì°¨ì§€í•˜ê³  ë”¥ëŸ¬ë‹ ì´ë¯¸ì§€ ë¶„ì•¼ì—ì„œ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆëŠ” ResNetì˜ êµ¬ì¡°ë¥¼ ì°¸ê³ í–ˆë‹¤.
    - ResNetì€ Residual Learningì„ ì´ìš©í•˜ëŠ”ë°, ìœ„ ê·¸ë¦¼ì˜ $F(x)$  (ì”ì°¨) + $x$ ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤. ResidualBlock í´ë˜ìŠ¤ì˜ forward ë©”ì„œë“œì—ì„œ ì´ êµ¬ì¡°ë¥¼ ë”°ëë‹¤.
    
    ```python
    class Net(nn.Module):
        def __init__(self, grid_size_X, grid_size_Y, action_size):
            super(Net, self).__init__()
            self.in_channels = 64
    
            self.conv = nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=3)
            self.bn = nn.BatchNorm2d(64)
            self.layer1 = self._make_layer(64, 2, stride=1)
            self.layer2 = self._make_layer(128, 2, stride=2)
            self.layer3 = self._make_layer(256, 2, stride=1)
            self.layer4 = self._make_layer(512, 2, stride=2)
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(512, action_size)
    
        def _make_layer(self, out_channels, blocks, stride=1):
            layers = []
            layers.append(ResidualBlock(self.in_channels, out_channels, stride))
            self.in_channels = out_channels
            for _ in range(1, blocks):
                layers.append(ResidualBlock(out_channels, out_channels))
            return nn.Sequential(*layers)
    
        def forward(self, x):
            out = F.relu(self.bn(self.conv(x)))
            out = self.layer1(out)
            out = self.layer2(out)
            out = self.layer3(out)
            out = self.layer4(out)
            out = self.avgpool(out)
            out = out.view(out.size(0), -1)
            # í…ì„œì˜ ì²« ë²ˆì§¸ (ì¸ë±ìŠ¤ê°€ 0) ì°¨ì› -ë°°ì¹˜ í¬ê¸°- ì€ ê³ ì •í•˜ê³  ë‚˜ë¨¸ì§€ ì°¨ì›ì˜ í¬ê¸°ë¥¼ ê³±í•´ 2ì°¨ì› í…ì„œë¡œ ë³€í™˜
            out = self.fc(out)
            return out
    ```
    
    Residual Learningì„ ì´ìš©í•œ Netì„ ì‚¬ìš©í•œ ê²½ìš°, ëª¨ë¸ì´ ë¬´ê±°ì›Œì§€ê³  ê·¸ë ‡ë‹¤ê³  í•´ì„œ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì§€ë„ ì•Šì•˜ê¸° ë•Œë¬¸ì— ìµœì¢… ëª¨ë¸ì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤.
    
10. Learning rate scheduler: lambdaLR / cyclicLR / StepLR
11. ëª¨ë¸ ì €ì¥(ì¶”ë¡  / í•™ìŠµ ì¬ê°œë¥¼ ìœ„í•´ ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸(checkpoint) ì €ì¥í•˜ê¸° & ë¶ˆëŸ¬ì˜¤ê¸°**)**
    - ì²´í¬í¬ì¸íŠ¸ ì €ì¥í•˜ê¸°
        
        ```python
        if epi % CHECKPOINT_INTERVAL == 0:
            checkpoint_path = f"checkpoint_{epi}.tar"
            save_checkpoint(agent, agent.optimizer, epi, score, checkpoint_path)
            print(f"Checkpoint saved at episode {epi} to {checkpoint_path}.")
        ```
        
    - ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
        
        ```python
        checkpoint_path = 'checkpoint_5000.tar'  # ì˜ˆ) 5000ë²ˆì§¸ ì—í”¼ì†Œë“œ ì²´í¬í¬ì¸íŠ¸
        agent, optimizer, start_epoch, last_loss = load_checkpoint(agent, optimizer, checkpoint_path)
        
        print(f"Checkpoint loaded from {checkpoint_path}. Starting from epoch {start_epoch}.")
        ```
***
# ìµœê³  ì„±ëŠ¥ì´ ë‚˜ì˜¨ ëª¨ë¸

- **Adam optimizerë¥¼ ì´ìš©í•œ ëª¨ë¸**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/1c873709-ed4b-4a75-ae8b-055a2c375a93/b665a0c8-4368-46fb-9d5c-ce4ea58fca86/Untitled.png)

- **RMSprop optimizerë¥¼ ì´ìš©í•œ ëª¨ë¸**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/1c873709-ed4b-4a75-ae8b-055a2c375a93/f457119c-a946-4bf8-a444-5fac91cb0cb0/Untitled.png)

***
# ë¬¸ì œ í•´ê²° ë° ê°œì„ í•œ ì 

- Trainì˜ #safe first click ë¶€ë¶„ ìˆ˜ì •
    - ê¸°ì¡´: ì²« ì¢Œí‘œë¶€í„° ëŒë©´ì„œ -1ì´ ì•„ë‹Œ ì²˜ìŒ ì¢Œí‘œ ì„ íƒ â†’ ì²˜ìŒ opení•˜ëŠ” ì¢Œí‘œê°€ í•­ìƒ ë¹„ìŠ·í•´ì§ (ì™¼ìª½ ìƒë‹¨ ë¶€ë¶„ì— ìœ„ì¹˜í•œ ì¢Œí‘œë“¤ë§Œ ì„ íƒí•˜ê²Œ ë¨)
    - ìˆ˜ì •: ì „ì²´ ì¢Œí‘œì—ì„œ ëœë¤ ì„ íƒ â†’ ì§€ë¢° ì„ íƒí–ˆìœ¼ë©´ ë‹¤ì‹œ ë‹¤ë¥¸ ì¢Œí‘œ ëœë¤ ì„ íƒ
    
    ```python
    while not done and time_step <= 71:
            time_step += 1
            if env.first_move:
                mine_state = env.minefield.flatten()
                first_action = random.randint(0, len(mine_state)-1)
                first_state = mine_state[first_action]
                while first_state == -1:
                    first_action = random.randint(0, len(mine_state)-1)
                    first_state = mine_state[first_action]
                action = first_action
                env.first_move = False
            else:
                action = agent.get_action(state)
    ```
    
- **score ì‚°ì • ë°©ì‹**
    - step ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ë¹„íš¨ìœ¨ì ì´ë¼ëŠ” ì  ê³ ë ¤
    
    ê° ì—í”¼ì†Œë“œì˜ ì „ì²´ ë³´ìƒë“¤ì˜ medianìœ¼ë¡œ ì„¤ì •í–ˆë”ë‹ˆ ëŒ€ë¶€ë¶„ ë™ì¼í•œ ê°’ì´ ë‚˜ì˜¤ë˜ ë¬¸ì œ
    
    â‡’ ê° ì—í”¼ì†Œë“œì˜ scoreì„ í•´ë‹¹ ì—í”¼ì†Œë“œì˜ ë³´ìƒ ì´í•©ìœ¼ë¡œ ìˆ˜ì •
    
- **ê°€ëŠ¥í•˜ë©´ ìƒìˆ˜ì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •**
    
     â‡’  ì½”ë“œì˜ ë…ë¦½ì„± í–¥ìƒ
    
- **íƒí—˜ ë¶€ì¡±**
    - epsilon_decay ê°’ì„ ëŠ˜ë¦¼â†’ epsilonì´ ë” ì²œì²œíˆ ê°ì†Œí•˜ë„ë¡
    - epsilon_min ê°’ì„ ì¤„ì—¬ ë‚˜ì¤‘ì—ëŠ” ì •ì±…ì— ë” ì˜ì¡´í•  ìˆ˜ ìˆë„ë¡
    - ì—¬ëŸ¬ ë°°ì¹˜ í¬ê¸° ì‹œë„
